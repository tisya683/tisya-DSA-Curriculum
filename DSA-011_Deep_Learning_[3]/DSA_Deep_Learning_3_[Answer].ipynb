{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZooFTWZo8eX"
      },
      "source": [
        "# DSA Deep Learning [3] - Hypertuning Our CNN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import statements\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "import io\n",
        "import base64\n",
        "from google.colab import files\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import display, HTML, Javascript\n",
        "from google.colab import output, files\n",
        "import zipfile"
      ],
      "metadata": {
        "id": "3HC-1Ua0Z33_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths to your CSV files\n",
        "train_csv_path = 'fer2013.csv'\n",
        "val_csv_path = 'train.csv'\n",
        "\n",
        "# Display the first few rows of the CSV file to check column names\n",
        "train_df = pd.read_csv(train_csv_path)\n",
        "print(train_df.head())\n",
        "print(train_df.columns)  # Print column names to verify\n",
        "\n",
        "# Display the first few rows of the CSV file to check column names\n",
        "val_df = pd.read_csv(val_csv_path)\n",
        "print(val_df.head())\n",
        "print(val_df.columns)  # Print column names to verify\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMc452lWw2o1",
        "outputId": "5de520f5-9065-40dc-cab0-6b9b94ee7b94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   emotion                                             pixels     Usage\n",
            "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
            "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
            "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
            "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
            "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training\n",
            "Index(['emotion', 'pixels', 'Usage'], dtype='object')\n",
            "   emotion                                             pixels\n",
            "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...\n",
            "1        0  151 150 147 155 148 133 111 140 170 174 182 15...\n",
            "2        2  231 212 156 164 174 138 161 173 182 200 106 38...\n",
            "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...\n",
            "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...\n",
            "Index(['emotion', 'pixels'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CSV data\n",
        "def load_data(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    X = np.array([np.fromstring(pixel_str, dtype=float, sep=' ') for pixel_str in df['pixels']])\n",
        "    X = X.reshape(-1, 48, 48, 1)  # Reshape to (n_samples, 48, 48, 1) for grayscale\n",
        "    X = np.repeat(X, 3, axis=-1)  # Convert grayscale to RGB format by repeating channels\n",
        "    X = X / 255.0  # Normalize pixel values\n",
        "    y = to_categorical(df['emotion'])  # One-hot encode the labels\n",
        "    return X, y\n",
        "\n",
        "# Load training and validation data\n",
        "X_train, y_train = load_data(train_csv_path)\n",
        "X_val, y_val = load_data(val_csv_path)"
      ],
      "metadata": {
        "id": "OvrsTPTKqzQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a TensorFlow data generator\n",
        "def create_generator(X, y, batch_size=64, shuffle=True):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(buffer_size=1024)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "# Create training and validation generators\n",
        "batch_size = 64\n",
        "train_generator = create_generator(X_train, y_train, batch_size=batch_size, shuffle=True)\n",
        "val_generator = create_generator(X_val, y_val, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "56i8nLOIn1uZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode one-hot encoded labels in `y_train` back to single integers\n",
        "y_train_labels = np.argmax(y_train, axis=1)\n",
        "\n",
        "# Calculate class weights to handle class imbalance\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train_labels),\n",
        "    y=y_train_labels\n",
        ")\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Data augmentation (only for training set)\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n"
      ],
      "metadata": {
        "id": "UilcIkNwZ9FN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained MobileNetV2 and add custom layers\n",
        "base_model = MobileNetV2(input_shape=(48, 48, 3), include_top=False, weights='imagenet')\n",
        "\n",
        "# Freeze all layers initially\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom layers on top\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(x)  # L2 regularization\n",
        "x = Dropout(0.5)(x)  # Dropout layer to reduce overfitting\n",
        "predictions = Dense(7, activation='softmax')(x)  # Output layer with 7 emotion classes\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Fine-tune the top layers\n",
        "for layer in base_model.layers[-20:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "vSsQsFGfaBp8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bd26759-66c8-4b92-8e88-de7e53fafc81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-5deb769f83ec>:2: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  base_model = MobileNetV2(input_shape=(48, 48, 3), include_top=False, weights='imagenet')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Callback to reduce learning rate when validation loss plateaus\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-5)\n",
        "\n",
        "# Early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Save the best model during training\n",
        "checkpoint = ModelCheckpoint('emotion_recognition_model_advanced.keras', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "# List of callbacks\n",
        "callbacks = [reduce_lr, early_stopping, checkpoint]\n"
      ],
      "metadata": {
        "id": "2Z6eIhYdpPiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=20,                           # Adjust based on your needs\n",
        "    validation_data=val_generator,\n",
        "    class_weight=class_weights_dict,\n",
        "    callbacks=callbacks\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xljs5Gn_pjYP",
        "outputId": "b9fa59dd-8501-4ebc-8673-72aa21751b5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m561/561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 41ms/step - accuracy: 0.2066 - loss: 4.3287 - val_accuracy: 0.3141 - val_loss: 3.5972 - learning_rate: 1.0000e-04\n",
            "Epoch 2/20\n",
            "\u001b[1m561/561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - accuracy: 0.3220 - loss: 3.2666 - val_accuracy: 0.3832 - val_loss: 2.8432 - learning_rate: 1.0000e-04\n",
            "Epoch 3/20\n",
            "\u001b[1m561/561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.3748 - loss: 2.6462 - val_accuracy: 0.4509 - val_loss: 2.3171 - learning_rate: 1.0000e-04\n",
            "Epoch 4/20\n",
            "\u001b[1m561/561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 22ms/step - accuracy: 0.4195 - loss: 2.1819 - val_accuracy: 0.5156 - val_loss: 1.9421 - learning_rate: 1.0000e-04\n",
            "Epoch 5/20\n",
            "\u001b[1m561/561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.4568 - loss: 1.8753 - val_accuracy: 0.5463 - val_loss: 1.6989 - learning_rate: 1.0000e-04\n",
            "Epoch 6/20\n",
            "\u001b[1m561/561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.4945 - loss: 1.6476 - val_accuracy: 0.5785 - val_loss: 1.5021 - learning_rate: 1.0000e-04\n",
            "Epoch 7/20\n",
            "\u001b[1m561/561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - accuracy: 0.5319 - loss: 1.4477 - val_accuracy: 0.6166 - val_loss: 1.3363 - learning_rate: 1.0000e-04\n",
            "Epoch 8/20\n",
            "\u001b[1m561/561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 16ms/step - accuracy: 0.5666 - loss: 1.2956 - val_accuracy: 0.6562 - val_loss: 1.1856 - learning_rate: 1.0000e-04\n",
            "Epoch 9/20\n",
            "\u001b[1m561/561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - accuracy: 0.6076 - loss: 1.1563 - val_accuracy: 0.6894 - val_loss: 1.0539 - learning_rate: 1.0000e-04\n",
            "Epoch 10/20\n",
            "\u001b[1m561/561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 16ms/step - accuracy: 0.6519 - loss: 1.0358 - val_accuracy: 0.7569 - val_loss: 0.8784 - learning_rate: 1.0000e-04\n",
            "Epoch 11/20\n",
            "\u001b[1m561/561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - accuracy: 0.6869 - loss: 0.9274 - val_accuracy: 0.7927 - val_loss: 0.7502 - learning_rate: 1.0000e-04\n",
            "Epoch 12/20\n",
            "\u001b[1m561/561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 20ms/step - accuracy: 0.7270 - loss: 0.8116 - val_accuracy: 0.8196 - val_loss: 0.6610 - learning_rate: 1.0000e-04\n",
            "Epoch 13/20\n",
            "\u001b[1m561/561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.7670 - loss: 0.7133 - val_accuracy: 0.8507 - val_loss: 0.5660 - learning_rate: 1.0000e-04\n",
            "Epoch 14/20\n",
            "\u001b[1m561/561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - accuracy: 0.7908 - loss: 0.6349 - val_accuracy: 0.8875 - val_loss: 0.4496 - learning_rate: 1.0000e-04\n",
            "Epoch 15/20\n",
            "\u001b[1m561/561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.8175 - loss: 0.5595 - val_accuracy: 0.8867 - val_loss: 0.4443 - learning_rate: 1.0000e-04\n",
            "Epoch 16/20\n",
            "\u001b[1m561/561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 20ms/step - accuracy: 0.8406 - loss: 0.5094 - val_accuracy: 0.9266 - val_loss: 0.3380 - learning_rate: 1.0000e-04\n",
            "Epoch 17/20\n",
            "\u001b[1m561/561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - accuracy: 0.8623 - loss: 0.4528 - val_accuracy: 0.9337 - val_loss: 0.3097 - learning_rate: 1.0000e-04\n",
            "Epoch 18/20\n",
            "\u001b[1m561/561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.8790 - loss: 0.4042 - val_accuracy: 0.9524 - val_loss: 0.2553 - learning_rate: 1.0000e-04\n",
            "Epoch 19/20\n",
            "\u001b[1m561/561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.8925 - loss: 0.3644 - val_accuracy: 0.9614 - val_loss: 0.2237 - learning_rate: 1.0000e-04\n",
            "Epoch 20/20\n",
            "\u001b[1m561/561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.8996 - loss: 0.3469 - val_accuracy: 0.9566 - val_loss: 0.2294 - learning_rate: 1.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the best saved model\n",
        "model_path = 'emotion_recognition_model_advanced.keras'\n",
        "\n",
        "# Load the best saved model if it exists\n",
        "if os.path.exists(model_path):\n",
        "    print(\"Loading the best saved model for predictions...\")\n",
        "    model = load_model(model_path)\n",
        "    print(\"Model loaded successfully.\")\n",
        "else:\n",
        "    print(\"No saved model found. Please train the model first.\")\n"
      ],
      "metadata": {
        "id": "d-onMA5frqg-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a968ae07-db15-4b2f-f7b5-37b94bbb5ee7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the best saved model for predictions...\n",
            "Model loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a dictionary for emotion labels based on FER2013 class order\n",
        "emotion_labels = {\n",
        "    0: \"Angry\",\n",
        "    1: \"Disgust\",\n",
        "    2: \"Fear\",\n",
        "    3: \"Happy\",\n",
        "    4: \"Sad\",\n",
        "    5: \"Surprise\",\n",
        "    6: \"Neutral\"\n",
        "}\n",
        "\n",
        "# Updated predict_emotion function to handle multiple faces\n",
        "def predict_emotion(frame, model):\n",
        "    # Convert to grayscale for face detection\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Load the face detection model (Haar Cascade)\n",
        "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "    # Detect multiple faces in the frame\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(48, 48))\n",
        "\n",
        "    # Process each detected face\n",
        "    for (x, y, w, h) in faces:\n",
        "        # Extract the face region from the frame\n",
        "        face = frame[y:y+h, x:x+w]\n",
        "\n",
        "        # Resize face region to 48x48, the input size expected by the model\n",
        "        face_resized = cv2.resize(face, (48, 48))\n",
        "\n",
        "        # Preprocess face (normalize and add batch dimension)\n",
        "        face_array = np.expand_dims(face_resized, axis=0) / 255.0  # Scale pixel values to [0, 1]\n",
        "\n",
        "        # Predict emotion\n",
        "        emotion_prediction = model.predict(face_array)\n",
        "        emotion = np.argmax(emotion_prediction)  # Get the emotion class with the highest probability\n",
        "\n",
        "        # Draw a circle around the face and add the emotion label\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "        emotion_label = emotion_labels[emotion]  # Map the predicted emotion index to label\n",
        "        cv2.putText(frame, emotion_label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "    return frame\n"
      ],
      "metadata": {
        "id": "closxhY9fpik"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JavaScript code to start the live webcam feed and capture image upon button click\n",
        "def start_webcam_feed():\n",
        "    js = \"\"\"\n",
        "    <script>\n",
        "        let videoElement = null;\n",
        "        let stream = null;\n",
        "\n",
        "        async function startVideo() {\n",
        "            if (!videoElement) {\n",
        "                videoElement = document.createElement('video');\n",
        "                videoElement.setAttribute('autoplay', '');\n",
        "                videoElement.setAttribute('playsinline', '');\n",
        "                document.body.appendChild(videoElement);\n",
        "                stream = await navigator.mediaDevices.getUserMedia({ video: true })\n",
        "                  .catch(err => {\n",
        "                      console.error('Webcam not accessible:', err);\n",
        "                      alert('Webcam not accessible. You can upload an image instead.');\n",
        "                  });\n",
        "                if (stream) {\n",
        "                    videoElement.srcObject = stream;\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        async function capturePhoto() {\n",
        "            if (!videoElement) {\n",
        "                alert(\"Webcam is not active!\");\n",
        "                return;\n",
        "            }\n",
        "            const canvas = document.createElement('canvas');\n",
        "            canvas.width = videoElement.videoWidth;\n",
        "            canvas.height = videoElement.videoHeight;\n",
        "            canvas.getContext('2d').drawImage(videoElement, 0, 0);\n",
        "\n",
        "            // Stop video feed\n",
        "            stream.getTracks().forEach(track => track.stop());\n",
        "            videoElement.remove();\n",
        "            videoElement = null;\n",
        "\n",
        "            // Convert the photo to base64 and send to Python\n",
        "            const dataUrl = canvas.toDataURL('image/jpeg');\n",
        "            google.colab.kernel.invokeFunction('notebook.get_webcam_image', [dataUrl], {});\n",
        "        }\n",
        "\n",
        "        // Add the start and capture buttons to the DOM\n",
        "        const startButton = document.createElement('button');\n",
        "        startButton.innerHTML = 'Start Webcam Feed';\n",
        "        startButton.onclick = startVideo;\n",
        "        document.body.appendChild(startButton);\n",
        "\n",
        "        const captureButton = document.createElement('button');\n",
        "        captureButton.innerHTML = 'Capture Photo';\n",
        "        captureButton.onclick = capturePhoto;\n",
        "        document.body.appendChild(captureButton);\n",
        "    </script>\n",
        "    \"\"\"\n",
        "    display(HTML(js))\n",
        "\n",
        "# Callback function to receive the captured image in Python\n",
        "def get_webcam_image(dataUrl):\n",
        "    img_data = base64.b64decode(dataUrl.split(\",\")[1])\n",
        "    img = Image.open(io.BytesIO(img_data))\n",
        "    img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
        "    processed_img = predict_emotion(img, model)\n",
        "    cv2_imshow(processed_img)\n",
        "\n",
        "# Function to enable file upload\n",
        "def enable_file_upload():\n",
        "    from google.colab import files\n",
        "    print(\"Upload an image if you prefer.\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    for file_name in uploaded.keys():\n",
        "        img = Image.open(file_name)\n",
        "        img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
        "        processed_img = predict_emotion(img, model)\n",
        "        cv2_imshow(processed_img)\n",
        "\n",
        "# Display the file upload button\n",
        "def display_file_upload_button():\n",
        "    js = \"\"\"\n",
        "    <script>\n",
        "        const uploadButton = document.createElement('button');\n",
        "        uploadButton.innerHTML = 'Upload an Image';\n",
        "        uploadButton.onclick = () => {\n",
        "            google.colab.kernel.invokeFunction('notebook.enable_file_upload', [], {});\n",
        "        };\n",
        "        document.body.appendChild(uploadButton);\n",
        "    </script>\n",
        "    \"\"\"\n",
        "    display(HTML(js))\n",
        "\n",
        "# Register the callbacks\n",
        "output.register_callback('notebook.get_webcam_image', get_webcam_image)\n",
        "output.register_callback('notebook.enable_file_upload', enable_file_upload)\n",
        "\n",
        "# Initialize the webcam feed, buttons, and file upload option\n",
        "start_webcam_feed()\n",
        "display_file_upload_button()\n"
      ],
      "metadata": {
        "id": "2LJojW38fr5q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}