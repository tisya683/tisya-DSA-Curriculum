{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZooFTWZo8eX"
      },
      "source": [
        "# DSA Deep Learning [3] - Hypertuning Our CNN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import statements\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "import io\n",
        "import base64\n",
        "from google.colab import files\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import display, HTML, Javascript\n",
        "from google.colab import output, files\n",
        "import zipfile"
      ],
      "metadata": {
        "id": "3HC-1Ua0Z33_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a dictionary for emotion labels based on FER2013 class order\n",
        "emotion_labels = {\n",
        "    0: \"Angry\",\n",
        "    1: \"Disgust\",\n",
        "    2: \"Fear\",\n",
        "    3: \"Happy\",\n",
        "    4: \"Sad\",\n",
        "    5: \"Surprise\",\n",
        "    6: \"Neutral\"\n",
        "}\n",
        "\n",
        "# Updated predict_emotion function to handle multiple faces\n",
        "def predict_emotion(frame, model):\n",
        "    # Convert to grayscale for face detection\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Load the face detection model (Haar Cascade)\n",
        "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "    # Detect multiple faces in the frame\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(48, 48))\n",
        "\n",
        "    # Process each detected face\n",
        "    for (x, y, w, h) in faces:\n",
        "        # Extract the face region from the frame\n",
        "        face = frame[y:y+h, x:x+w]\n",
        "\n",
        "        # Resize face region to 48x48, the input size expected by the model\n",
        "        face_resized = cv2.resize(face, (48, 48))\n",
        "\n",
        "        # Preprocess face (normalize and add batch dimension)\n",
        "        face_array = np.expand_dims(face_resized, axis=0) / 255.0  # Scale pixel values to [0, 1]\n",
        "\n",
        "        # Predict emotion\n",
        "        emotion_prediction = model.predict(face_array)\n",
        "        emotion = np.argmax(emotion_prediction)  # Get the emotion class with the highest probability\n",
        "\n",
        "        # Draw a circle around the face and add the emotion label\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "        emotion_label = emotion_labels[emotion]  # Map the predicted emotion index to label\n",
        "        cv2.putText(frame, emotion_label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "    return frame\n"
      ],
      "metadata": {
        "id": "closxhY9fpik"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JavaScript code to start the live webcam feed and capture image upon button click\n",
        "def start_webcam_feed():\n",
        "    js = \"\"\"\n",
        "    <script>\n",
        "        let videoElement = null;\n",
        "        let stream = null;\n",
        "\n",
        "        async function startVideo() {\n",
        "            if (!videoElement) {\n",
        "                videoElement = document.createElement('video');\n",
        "                videoElement.setAttribute('autoplay', '');\n",
        "                videoElement.setAttribute('playsinline', '');\n",
        "                document.body.appendChild(videoElement);\n",
        "                stream = await navigator.mediaDevices.getUserMedia({ video: true })\n",
        "                  .catch(err => {\n",
        "                      console.error('Webcam not accessible:', err);\n",
        "                      alert('Webcam not accessible. You can upload an image instead.');\n",
        "                  });\n",
        "                if (stream) {\n",
        "                    videoElement.srcObject = stream;\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        async function capturePhoto() {\n",
        "            if (!videoElement) {\n",
        "                alert(\"Webcam is not active!\");\n",
        "                return;\n",
        "            }\n",
        "            const canvas = document.createElement('canvas');\n",
        "            canvas.width = videoElement.videoWidth;\n",
        "            canvas.height = videoElement.videoHeight;\n",
        "            canvas.getContext('2d').drawImage(videoElement, 0, 0);\n",
        "\n",
        "            // Stop video feed\n",
        "            stream.getTracks().forEach(track => track.stop());\n",
        "            videoElement.remove();\n",
        "            videoElement = null;\n",
        "\n",
        "            // Convert the photo to base64 and send to Python\n",
        "            const dataUrl = canvas.toDataURL('image/jpeg');\n",
        "            google.colab.kernel.invokeFunction('notebook.get_webcam_image', [dataUrl], {});\n",
        "        }\n",
        "\n",
        "        // Add the start and capture buttons to the DOM\n",
        "        const startButton = document.createElement('button');\n",
        "        startButton.innerHTML = 'Start Webcam Feed';\n",
        "        startButton.onclick = startVideo;\n",
        "        document.body.appendChild(startButton);\n",
        "\n",
        "        const captureButton = document.createElement('button');\n",
        "        captureButton.innerHTML = 'Capture Photo';\n",
        "        captureButton.onclick = capturePhoto;\n",
        "        document.body.appendChild(captureButton);\n",
        "    </script>\n",
        "    \"\"\"\n",
        "    display(HTML(js))\n",
        "\n",
        "# Callback function to receive the captured image in Python\n",
        "def get_webcam_image(dataUrl):\n",
        "    img_data = base64.b64decode(dataUrl.split(\",\")[1])\n",
        "    img = Image.open(io.BytesIO(img_data))\n",
        "    img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
        "    processed_img = predict_emotion(img, model)\n",
        "    cv2_imshow(processed_img)\n",
        "\n",
        "# Register the callbacks\n",
        "output.register_callback('notebook.get_webcam_image', get_webcam_image)\n",
        "\n",
        "# Initialize the webcam feed, buttons, and file upload option\n",
        "start_webcam_feed()\n",
        "\n"
      ],
      "metadata": {
        "id": "2LJojW38fr5q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}