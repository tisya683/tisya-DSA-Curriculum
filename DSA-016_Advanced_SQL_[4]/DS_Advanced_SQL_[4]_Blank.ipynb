{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbZE3YI-7iff"
   },
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ehQDLyGD3Pm2",
    "outputId": "5b10ab6c-a7e9-4c3e-a82d-0bbfb3fc466e"
   },
   "outputs": [],
   "source": [
    "# Uncomment once and run the file to get access to all the necessary libraries\n",
    "%pip install -r requirements.txt\n",
    "# %pip install --upgrade openai langchain\n",
    "# !apt-get install -y poppler-utils\n",
    "# !apt-get install -y tesseract-ocr\n",
    "# !apt-get install -y libmagic1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_FeiYQGxt56B",
    "outputId": "f6534645-fcdb-417f-ddae-2c7d5582f95b"
   },
   "outputs": [],
   "source": [
    "# Latest update to the langchain package causing issue here\n",
    "!pip uninstall httpx -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QgzwYb66t-iE",
    "outputId": "6aee9b0b-5d75-49fa-db5b-729029f68654"
   },
   "outputs": [],
   "source": [
    "# Reinstall to this version to enable langchain to work properly\n",
    "!pip install httpx==0.27.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2t2vTlQF6h_3",
    "outputId": "93c7f057-7c17-49fc-e378-61f603c03842"
   },
   "outputs": [],
   "source": [
    "!pip show httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gWzdqTXndL1F",
    "outputId": "6615b0a8-946d-4fa0-f979-ffce676b2ee7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import bs4\n",
    "import ast\n",
    "import fitz\n",
    "import uuid\n",
    "import json\n",
    "import base64\n",
    "import markdown\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "from PyPDF2 import PdfMerger\n",
    "from base64 import b64decode\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.documents.elements import Text, Image\n",
    "from typing_extensions import Annotated, TypedDict, Sequence, List\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain.text_splitter  import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "\n",
    "from langchain_community.document_loaders import(\n",
    "  PyPDFLoader,\n",
    "  Docx2txtLoader,\n",
    "  UnstructuredPDFLoader,\n",
    "  WebBaseLoader,\n",
    "  UnstructuredMarkdownLoader,\n",
    "  UnstructuredWordDocumentLoader,\n",
    "  TextLoader,\n",
    "  UnstructuredPDFLoader\n",
    ")\n",
    "\n",
    "from chromadb.config import Settings\n",
    "from chromadb.api.types import Embedding\n",
    "from langchain.schema import BaseMessage\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.schema.document import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_community.vectorstores import Chroma, InMemoryVectorStore\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "load_dotenv(\"template.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqZ4gK1XXSz3"
   },
   "source": [
    "# SQL Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "eERkMAjB127U",
    "outputId": "d4380f79-b9c5-4933-c1ee-e993dc71cac5"
   },
   "outputs": [],
   "source": [
    "# Let's connect to the database again\n",
    "db = SQLDatabase.from_uri(\"sqlite:///brain_tumor_mri.db\")\n",
    "print(db.dialect)\n",
    "print(db.get_usable_table_names())\n",
    "db.run(\"SELECT DISTINCT label FROM mri_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "muMyEhMR4eko",
    "outputId": "ae046a06-4304-4b4c-bd82-dc6778131216"
   },
   "outputs": [],
   "source": [
    "# Tracing via Langsmith\n",
    "trace = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "langsmith = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Build a GPT model\n",
    "gpt = ChatOpenAI(\n",
    "    model = \"gpt-4-turbo\",\n",
    "    temperature=0,\n",
    "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "# Incase we do any embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# Little test\n",
    "response = gpt.invoke(\"Why Abhi bang the table?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s4hKAAe1c0ZE"
   },
   "outputs": [],
   "source": [
    " # LangGraph create our workflow!\n",
    "class SqlState(TypedDict):\n",
    "    question: str\n",
    "    query: str\n",
    "    result: str\n",
    "    answer: str\n",
    "\n",
    "query_prompt_template = hub.pull(\"langchain-ai/sql-query-system-prompt\")\n",
    "\n",
    "# Make sure we only have one message\n",
    "assert len(query_prompt_template.messages) == 1\n",
    "\n",
    "# query_prompt_template.messages[0].pretty_print()\n",
    "\n",
    "# Create our personalised pydantic model\n",
    "class QueryOutput(TypedDict):\n",
    "    \"\"\"Generated SQL query.\"\"\"\n",
    "    query: Annotated[str, ..., \"Syntactically valid SQL query.\"] #This serves as an hint to what kind of query is acceptable!\n",
    "\n",
    "def write_query(state: SqlState):\n",
    "    \"\"\"Generate SQL query to fetch information.\"\"\"\n",
    "    prompt = query_prompt_template.invoke(\n",
    "        {\n",
    "            \"dialect\": db.dialect,\n",
    "            \"top_k\": 5,\n",
    "            \"table_info\": db.get_table_info(),\n",
    "            \"input\": state[\"question\"],\n",
    "        }\n",
    "    )\n",
    "    structured_llm = gpt.with_structured_output(QueryOutput)\n",
    "    result = structured_llm.invoke(prompt)\n",
    "    return {\"query\": result[\"query\"]}\n",
    "\n",
    "# print(write_query({\"question\": \"How many rows are there?\"}))\n",
    "\n",
    "def execute_query(state: SqlState):\n",
    "    \"\"\"Execute SQL query\"\"\"\n",
    "    execute_query_tool = QuerySQLDataBaseTool(db=db)\n",
    "    return {\"result\": execute_query_tool.invoke(state[\"query\"])}\n",
    "\n",
    "# print(execute_query(write_query({\"question\": \"How many Employees are there?\"})))\n",
    "\n",
    "def generate_answer(state: SqlState):\n",
    "    \"\"\"Answer question using retrieved information as context.\"\"\"\n",
    "    prompt = (\n",
    "        \"Given the following user question, corresponding SQL query, \"\n",
    "        \"and SQL result, answer the user question.\\n\\n\"\n",
    "        f'Question: {state[\"question\"]}\\n'\n",
    "        f'SQL Query: {state[\"query\"]}\\n'\n",
    "        f'SQL Result: {state[\"result\"]}'\n",
    "    )\n",
    "    response = gpt.invoke(prompt)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "graph_builder = StateGraph(SqlState).add_sequence(\n",
    "    [write_query, execute_query, generate_answer]\n",
    ")\n",
    "graph_builder.add_edge(START, \"write_query\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "PZszjtO7jVxR",
    "outputId": "ab0cae00-1eaf-44c1-ec9a-250a70c13c3b"
   },
   "outputs": [],
   "source": [
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EFki_HDVnXxR"
   },
   "outputs": [],
   "source": [
    "# Save your money don't need to run this\n",
    "# graph.invoke({\"question\":\"How many no tumor patients are there?\"})['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GedNauoHgdzo"
   },
   "outputs": [],
   "source": [
    "# Function to process the question and get the answer\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory, interrupt_before=[\"execute_query\"])\n",
    "\n",
    "def answer_question():\n",
    "  question = {'question': input(\"What is your SQL related question? \")}\n",
    "  config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "  # Run the graph\n",
    "  for step in graph.stream(question, config, stream_mode=\"updates\"):\n",
    "      # print(step)\n",
    "      pass  # We can pass this to make it neater\n",
    "\n",
    "  # Retrieve the final state\n",
    "  try:\n",
    "    user_approval = input(\"Do you want to execute query? (yes/no): \")\n",
    "  except Exception:\n",
    "    user_approval = \"no\"\n",
    "\n",
    "  if user_approval.lower() == \"yes\":\n",
    "    for state in graph.stream(None, config, stream_mode=\"values\"):\n",
    "        pass\n",
    "\n",
    "    final_output = state.get(\"answer\")\n",
    "    print(final_output)\n",
    "\n",
    "  else:\n",
    "    for state in graph.stream(None, config, stream_mode=\"values\"):\n",
    "        pass\n",
    "\n",
    "    generated_query = state.get(\"query\")\n",
    "    return print(f\"Operation cancelled by user. Here is the query: {generated_query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W7UR7VeRl_9W"
   },
   "outputs": [],
   "source": [
    "# Save your money don't need to run this\n",
    "# answer_question()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "JYf9E_Gbxdpv",
    "outputId": "c5ee3130-129b-488e-e62b-5c9102010f6c"
   },
   "outputs": [],
   "source": [
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXhbtBcLqU4t"
   },
   "source": [
    "# RAG system\n",
    "## So far we have only handled PDF with only words what if we want to deal with PDF files that are multimedia?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x29JaCDpXJIp"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LOgCJPvHhjWe"
   },
   "outputs": [],
   "source": [
    "# Merge the PDF together\n",
    "pdf_files = [\"cancer1.pdf\", \"cancer2.pdf\"]\n",
    "merger = PdfMerger()\n",
    "\n",
    "for pdf in pdf_files:\n",
    "    merger.append(pdf)\n",
    "\n",
    "output_path = \"merged.pdf\"\n",
    "merger.write(output_path)\n",
    "merger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "collapsed": true,
    "id": "BnVJXKrKU7V3",
    "outputId": "b12d8455-b5d8-4ad8-cc29-cbfc431ba539"
   },
   "outputs": [],
   "source": [
    "# Run this to get the our chunks\n",
    "# We will skip this step due to the long waiting time\n",
    "'''\n",
    "chunks = partition_pdf(\n",
    "    filename='merged.pdf',\n",
    "    infer_table_structure=True,            # extract tables\n",
    "    strategy=\"hi_res\",                     # mandatory to infer tables\n",
    "\n",
    "    extract_image_block_types=[\"Image\"],   # Add 'Table' to list to extract image of tables\n",
    "    image_output_dir_path=output_path,   # if None, images and tables will saved in base64\n",
    "\n",
    "    extract_image_block_to_payload=True,   # if true, will extract base64 for API usage\n",
    "\n",
    "    chunking_strategy=\"by_title\",          # or 'basic'\n",
    "    max_characters=10000,                  # defaults to 500\n",
    "    combine_text_under_n_chars=2000,       # defaults to 0\n",
    "    new_after_n_chars=6000,\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9xsJdRflRLc1",
    "outputId": "4f399ed6-b643-416d-f537-7c3bf6ee9254"
   },
   "outputs": [],
   "source": [
    "output_file = \"chunks.pkl\"\n",
    "\n",
    "'''\n",
    "# For saving the chunks to a file\n",
    "output_file = \"chunks.pkl\"\n",
    "with open(output_file, \"wb\") as file:  # \"wb\" for write binary\n",
    "    pickle.dump(chunks, file)\n",
    "\n",
    "print(f\"Chunks saved to {output_file}\")\n",
    "'''\n",
    "\n",
    "# Load the chunks back\n",
    "with open(output_file, \"rb\") as file:  # \"rb\" for read binary\n",
    "    chunks = pickle.load(file)\n",
    "\n",
    "print(f\"Chunks loaded successfully. Type of first element: {type(chunks[0])}\") # Should see only composite elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFmu0yKLwI_f"
   },
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "qUOk2qHw3QbA"
   },
   "outputs": [],
   "source": [
    "# Overview\n",
    "# pprint(chunks)\n",
    "\n",
    "# Chunk contents\n",
    "# pprint(chunks[7].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WUzNM15Odcuy",
    "outputId": "1be3da98-ce9b-4fc7-eae5-809a2087d57f"
   },
   "outputs": [],
   "source": [
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sJVpJbx71Yc6",
    "outputId": "66a96311-b290-4b43-d03a-d30819ee412e"
   },
   "outputs": [],
   "source": [
    "# We get 2 types of elements from the partition_pdf function\n",
    "print(set([str(type(i)) for i in chunks]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "NHvVvPSJdpuc",
    "outputId": "b5e71e82-7430-4b98-9197-acf6435911f8"
   },
   "outputs": [],
   "source": [
    "pprint(chunks[0].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "QhL74t5-13-M",
    "outputId": "9428adea-e340-4aac-8f6f-735ad78242ba"
   },
   "outputs": [],
   "source": [
    "chunks[7].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "MFELMlNN0-SA",
    "outputId": "2cded344-536a-4be2-8b63-ecef8bbf6f3a"
   },
   "outputs": [],
   "source": [
    "# pprint(chunks[3])\n",
    "pprint(chunks[3].metadata.orig_elements)\n",
    "\n",
    "print(\"-------------------------\")\n",
    "\n",
    "# pprint(chunks[3].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "HOyhsErL2wUr",
    "outputId": "cecd2767-e0dc-44f9-aeb7-73a8fd8dc278"
   },
   "outputs": [],
   "source": [
    "# Take a sneak peak at the text data\n",
    "elements = chunks[3].metadata.orig_elements\n",
    "# print(elements)\n",
    "\n",
    "chunk_texts = [e for e in elements if 'NarrativeText' in str(type(e))]\n",
    "# print(chunk_texts)\n",
    "\n",
    "for text in chunk_texts:\n",
    "  # pprint(text.metadata)\n",
    "  pprint(str(text))\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0kDyo-fU8u65",
    "outputId": "77343bb4-f7c9-4a94-e5a6-f2888c53bfdd"
   },
   "outputs": [],
   "source": [
    "# Separate tables from texts\n",
    "tables = []\n",
    "texts = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    if \"Table\" in str(type(chunk)):\n",
    "        tables.append(chunk)\n",
    "\n",
    "    if \"CompositeElement\" in str(type((chunk))):\n",
    "        texts.append(chunk)\n",
    "\n",
    "print(len(tables))\n",
    "print(tables[0:1])\n",
    "print(tables[0])\n",
    "print(\"--------------\")\n",
    "print(len(texts))\n",
    "print(texts[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "heA7RDMPdps1",
    "outputId": "4d92cbbf-ce87-424f-c0af-5d25be1a81ab"
   },
   "outputs": [],
   "source": [
    "# Get the images from the CompositeElement objects\n",
    "def get_images_base64(chunks):\n",
    "  images_b64 = []\n",
    "  for chunk in chunks:\n",
    "    if \"CompositeElement\" in str(type(chunk)):\n",
    "      chunk_els = chunk.metadata.orig_elements\n",
    "      for el in chunk_els:\n",
    "        if \"Image\" in str(type(el)):\n",
    "            images_b64.append(el.metadata.image_base64)\n",
    "  return images_b64\n",
    "\n",
    "images = get_images_base64(chunks)\n",
    "pprint(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "lSK8c_DkdplY",
    "outputId": "19e81952-4166-458c-fb7b-25f51d34ae0e"
   },
   "outputs": [],
   "source": [
    "# Decode the base64 string to binary\n",
    "# Display the image\n",
    "\n",
    "def display_base64_image(base64_code):\n",
    "    image_data = base64.b64decode(base64_code)\n",
    "    display(Image(data=image_data))\n",
    "\n",
    "display_base64_image(images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTfOrZmH9c5n"
   },
   "source": [
    "## Summarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "QO7DPW-0_m9X",
    "outputId": "2e572417-aa7d-43c0-c2c4-0a4244e7faf2"
   },
   "outputs": [],
   "source": [
    "pprint(tables[0].to_dict())\n",
    "print(\"--------------\")\n",
    "pprint(texts[0].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRmpfg9Adph3"
   },
   "outputs": [],
   "source": [
    "# Text summariser\n",
    "prompt_text = \"\"\"\n",
    "You are an assistant tasked with summarizing tables and text.\n",
    "Give a concise summary of the table or text.\n",
    "\n",
    "Respond only with the summary, no additionnal comment.\n",
    "Do not start your message by saying \"Here is a summary\" or anything like that.\n",
    "Just give the summary as it is.\n",
    "\n",
    "Table or text chunk: {element}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Summary chain\n",
    "\n",
    "\n",
    "# Summarize text, concurrency limits the number of task running at the same time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "XdikiIpMFjEq",
    "outputId": "bf2d2043-9239-4502-db30-0203faa09942"
   },
   "outputs": [],
   "source": [
    "# Check text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "JCL9mqB6hUQo",
    "outputId": "78b2bbdb-ed43-4ee2-83aa-f956e8b81242"
   },
   "outputs": [],
   "source": [
    "# Check first index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J_GrjdwfEwQT"
   },
   "outputs": [],
   "source": [
    "# Summarize tables\n",
    "tables_html = [table.metadata.text_as_html for table in tables]\n",
    "table_summaries = summarize_chain.batch(tables_html, {\"max_concurrency\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "CrZ0pBdNfXry",
    "outputId": "63f82c1e-c360-4fe6-f83f-b5df5a5607a0"
   },
   "outputs": [],
   "source": [
    "# Check table \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "DGDjhKtihUPE",
    "outputId": "2e264089-017a-4731-9df8-b559ae7bbbcb"
   },
   "outputs": [],
   "source": [
    "# Image summariser\n",
    "prompt_template = \"\"\"\n",
    "  Describe the image in detail. For context,\n",
    "  the image is part of a research paper explaining the meidcal treatment\n",
    "  of brain cancer. Be specific about graphs such as bar plots if any\n",
    "  or diagrams showing the human anatomy.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"user\",\n",
    "        [\n",
    "            {\"type\": \"text\", \"text\": prompt_template},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": \"data:image/jpeg;base64,{image}\"},\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "chain = prompt | gpt | StrOutputParser()\n",
    "\n",
    "\n",
    "image_summaries = chain.batch(images)\n",
    "pprint(image_summaries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zlAiqsDKhqMW"
   },
   "outputs": [],
   "source": [
    "# Set up persistence directory for Chroma\n",
    "persist_directory = \"/content/db\"\n",
    "\n",
    "# The vectorstore to use to index the chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"multi_modal_rag\",\n",
    "    embedding_function=embeddings,\n",
    "    client_settings=Settings(persist_directory=persist_directory)\n",
    ")\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7LN0SL9BoKx"
   },
   "outputs": [],
   "source": [
    "# Add texts\n",
    "doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "# print(doc_ids[0:2])\n",
    "\n",
    "summary_texts = [\n",
    "    Document(page_content=summary, metadata={id_key: doc_ids[i]}) for i, summary in enumerate(text_summaries)\n",
    "]\n",
    "\n",
    "retriever.vectorstore.add_documents(summary_texts)\n",
    "retriever.docstore.mset(list(zip(doc_ids, texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "2hbNQ9wiMQEd",
    "outputId": "c8bd0f44-800a-423d-a864-5cf6af768593"
   },
   "outputs": [],
   "source": [
    "results = retriever.invoke(\"How to treat meningioma?\")\n",
    "results[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GaQd0Pmyhp-l"
   },
   "outputs": [],
   "source": [
    "# Add tables\n",
    "table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "summary_tables = [\n",
    "    Document(page_content=summary, metadata={id_key: table_ids[i]}) for i, summary in enumerate(table_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_tables)\n",
    "retriever.docstore.mset(list(zip(table_ids, tables)))\n",
    "\n",
    "# Add image summaries\n",
    "img_ids = [str(uuid.uuid4()) for _ in images]\n",
    "summary_img = [\n",
    "    Document(page_content=summary, metadata={id_key: img_ids[i]}) for i, summary in enumerate(image_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_img)\n",
    "retriever.docstore.mset(list(zip(img_ids, images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wB85tRrakNvL"
   },
   "outputs": [],
   "source": [
    "# Retrieve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "collapsed": true,
    "id": "1FZWeI8gNlL3",
    "outputId": "ae612adf-996d-4b18-a1f0-a3a786d828d8"
   },
   "outputs": [],
   "source": [
    "# Display the images!\n",
    "\n",
    "def display_base64_image(base64_code):\n",
    "  image_data = base64.b64decode(base64_code)\n",
    "  display(Image(data=image_data))\n",
    "\n",
    "for i in range(0,4):\n",
    "  display_base64_image(docs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwRhDytBdAFn"
   },
   "source": [
    "## Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cm4qJ61Yc5SG",
    "outputId": "da86a24a-ab46-4495-bc09-8153816b5422"
   },
   "outputs": [],
   "source": [
    "# Creating the image and text answer\n",
    "\n",
    "\n",
    "# Testing\n",
    "\n",
    "\n",
    "# Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YV-kUeTBdsSO"
   },
   "outputs": [],
   "source": [
    "def build_prompt(kwargs):\n",
    "  docs_by_type = kwargs[\"context\"]\n",
    "  user_question = kwargs[\"question\"]\n",
    "\n",
    "  context_text = \"\"\n",
    "  if len(docs_by_type[\"texts\"]) > 0:\n",
    "      for text_element in docs_by_type[\"texts\"]:\n",
    "          context_text += text_element.text\n",
    "\n",
    "  # construct prompt with context (including images)\n",
    "  prompt_template = f\"\"\"\n",
    "  Answer the question to the best of your abilities based only on the following\n",
    "  context, which can include text, tables, and the image below.\n",
    "  Context: {context_text}\n",
    "  Question: {user_question}\n",
    "  \"\"\"\n",
    "\n",
    "  prompt_content = [{\"type\": \"text\", \"text\": prompt_template}]\n",
    "\n",
    "  if len(docs_by_type[\"images\"]) > 0:\n",
    "      for image in docs_by_type[\"images\"]:\n",
    "          prompt_content.append(\n",
    "              {\n",
    "                  \"type\": \"image_url\",\n",
    "                  \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "              }\n",
    "          )\n",
    "\n",
    "  return ChatPromptTemplate.from_messages([HumanMessage(content=prompt_content),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7QpCUVQLfL9L"
   },
   "outputs": [],
   "source": [
    "# First chain \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7tpzkaSafc83",
    "outputId": "3bfb7fc0-fa7d-4b50-cba1-bd4d7423818f"
   },
   "outputs": [],
   "source": [
    "print(chain.invoke(\"How to treat meningioma?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jcrlaekJkmZ8"
   },
   "outputs": [],
   "source": [
    "# Chain with sources/ images and tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hAynCXlnlQsZ",
    "outputId": "a6e2728b-7ef6-4fb8-d3ad-42b8ce5dc5b3"
   },
   "outputs": [],
   "source": [
    "response = chain_with_sources.invoke(\"Can you show a picture of the brain\")\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "0KXYveVDpt3g",
    "outputId": "79e1c8f3-b4ce-428e-df84-004f9b3cf603"
   },
   "outputs": [],
   "source": [
    "# response = chain_with_sources.invoke(\"Show me a picture of the brain\")\n",
    "# response = chain_with_sources.invoke(\"Show me a picture of craniotomy\")\n",
    "# response = chain_with_sources.invoke(\"How to treat meningioma\")\n",
    "\n",
    "if response['context']['images'] is not None:\n",
    "  for image in response['context']['images']:\n",
    "      display_base64_image(image)\n",
    "else:\n",
    "  print(\"Response:\", response['response'])\n",
    "\n",
    "\n",
    "print(\"\\n\\nContext:\")\n",
    "for text in response['context']['texts']:\n",
    "    print(text.text)\n",
    "    print(\"Page number: \", text.metadata.page_number)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "for image in response['context']['images']:\n",
    "    display_base64_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 627
    },
    "collapsed": true,
    "id": "2lpSAl5Sgvj0",
    "outputId": "3871b303-494d-4dc7-c206-b03d0bab5b71"
   },
   "outputs": [],
   "source": [
    "if response['context']['images'] is not None:\n",
    "  print(\"Response:\", response['response'])\n",
    "  display_base64_image(response['context']['images'][0])\n",
    "else:\n",
    "  print(\"Response:\", response['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 627
    },
    "collapsed": true,
    "id": "sfXflsHHtRLo",
    "outputId": "58a9b2fb-da38-468e-bd63-889a61021c7d"
   },
   "outputs": [],
   "source": [
    "source = chain_with_sources.invoke(\"Show me a picture of the brain\")\n",
    "print(response['response'])\n",
    "\n",
    "display_base64_image(source['context']['images'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fl8eQ8oMXByN"
   },
   "source": [
    "## Rag langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "collapsed": true,
    "id": "XHMqQh8qR8ej",
    "outputId": "92898572-6abb-4fa6-882b-fd9898ce9788"
   },
   "outputs": [],
   "source": [
    "from typing import TypedDict, List, Dict, Any\n",
    "from langgraph.graph.state import StateGraph, START\n",
    "\n",
    "class RagState(TypedDict):\n",
    "    \"\"\"Represents the state of our graph.\"\"\"\n",
    "    question: str\n",
    "    response: str\n",
    "    images: List[str]\n",
    "\n",
    "# Generation node\n",
    "    # \"\"\"Generate a response using the chain_with_sources.\"\"\"\n",
    "\n",
    "\n",
    "def showcase_answers(state: RagState):\n",
    "    \"\"\"Showcase all answers, including the response and images.\"\"\"\n",
    "    # print(\"Generated Response:\")\n",
    "    # print(state[\"response\"])\n",
    "\n",
    "    if state[\"images\"]:\n",
    "        for i, image in enumerate(state[\"images\"]):\n",
    "            print(f\"Image {i + 1}:\")\n",
    "            display_base64_image(image)\n",
    "    else:\n",
    "        print(\"No diagrams available.\")\n",
    "\n",
    "    return state\n",
    "\n",
    "# Graph building\n",
    "\n",
    "\n",
    "result = rag.invoke({\"question\": \"Can you show a picture of the brain?\"}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "RGMcldheNTsc",
    "outputId": "0bcb16a0-280b-4754-9ad9-bbc3f70f3bcc"
   },
   "outputs": [],
   "source": [
    "display(Image(rag.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7L7_TmWLW86H"
   },
   "source": [
    "# Parent Graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDdochbcioiS"
   },
   "source": [
    "## New SQL graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A0uR_wOsdmBV",
    "outputId": "9ef49c87-a6a2-416d-bc5b-74b67f8587de"
   },
   "outputs": [],
   "source": [
    "# LangGraph create our workflow!\n",
    "class SqlState(TypedDict):\n",
    "    question: str\n",
    "    query: str\n",
    "    result: str\n",
    "    answer: str\n",
    "\n",
    "query_prompt_template = hub.pull(\"langchain-ai/sql-query-system-prompt\")\n",
    "\n",
    "# Make sure we only have one message\n",
    "assert len(query_prompt_template.messages) == 1\n",
    "\n",
    "# query_prompt_template.messages[0].pretty_print()\n",
    "\n",
    "# Create our personalised pydantic model\n",
    "class QueryOutput(TypedDict):\n",
    "    \"\"\"Generated SQL query.\"\"\"\n",
    "    query: Annotated[str, ..., \"Syntactically valid SQL query.\"]\n",
    "\n",
    "def write_query(state: SqlState):\n",
    "    \"\"\"Generate SQL query to fetch information.\"\"\"\n",
    "    prompt = query_prompt_template.invoke(\n",
    "        {\n",
    "            \"dialect\": db.dialect,\n",
    "            \"top_k\": 5,\n",
    "            \"table_info\": db.get_table_info(),\n",
    "            \"input\": state[\"question\"],\n",
    "        }\n",
    "    )\n",
    "    structured_llm = gpt.with_structured_output(QueryOutput)\n",
    "    result = structured_llm.invoke(prompt)\n",
    "    return {\"query\": result[\"query\"]}\n",
    "\n",
    "# print(write_query({\"question\": \"How many rows are there?\"}))\n",
    "\n",
    "def execute_query(state: SqlState):\n",
    "    \"\"\"Execute SQL query\"\"\"\n",
    "    execute_query_tool = QuerySQLDataBaseTool(db=db)\n",
    "    return {\"result\": execute_query_tool.invoke(state[\"query\"])}\n",
    "\n",
    "# print(execute_query(write_query({\"question\": \"How many Employees are there?\"})))\n",
    "\n",
    "def generate_answer(state: SqlState):\n",
    "    \"\"\"Answer question using retrieved information as context.\"\"\"\n",
    "    prompt = (\n",
    "        \"Given the following user question, corresponding SQL query, \"\n",
    "        \"and SQL result, answer the user question.\\n\\n\"\n",
    "        f'Question: {state[\"question\"]}\\n'\n",
    "        f'SQL Query: {state[\"query\"]}\\n'\n",
    "        f'SQL Result: {state[\"result\"]}'\n",
    "    )\n",
    "    response = gpt.invoke(prompt)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "sql_builder = StateGraph(SqlState).add_sequence(\n",
    "  [write_query, execute_query, generate_answer]\n",
    ")\n",
    "\n",
    "memory = MemorySaver()\n",
    "sql_builder.add_edge(START, \"write_query\")\n",
    "sql = sql_builder.compile(checkpointer=memory)\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "print(sql.invoke({\"question\": \"How many patients are there?\"}, config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "88ELQYOvi4Hf",
    "outputId": "6155a3af-0dd9-41ad-d92d-c7f157e350bc"
   },
   "outputs": [],
   "source": [
    "display(Image(sql.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2-8jr_v20ey"
   },
   "source": [
    "## New RAG Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I2WSXHDi2zgf",
    "outputId": "c2b8907e-c5b9-482e-fb88-929183aee77e"
   },
   "outputs": [],
   "source": [
    "class RagState(TypedDict):\n",
    "    \"\"\"Represents the state of our graph.\"\"\"\n",
    "    question: str\n",
    "    answer: str\n",
    "    images: List[str]\n",
    "\n",
    "def generation(state: RagState):\n",
    "    \"\"\"Generate a response using the chain_with_sources.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    response = chain_with_sources.invoke(question)\n",
    "\n",
    "    text = response.get('response', \"No response generated\")\n",
    "\n",
    "    images = response.get('context', {}).get('images', [])\n",
    "    images = images if images else \"No diagrams\"\n",
    "\n",
    "    return {\n",
    "        \"answer\": text,\n",
    "        \"images\": images,\n",
    "    }\n",
    "\n",
    "rag_graph = StateGraph(RagState)\n",
    "rag_graph.add_sequence([generation])\n",
    "rag_graph.add_edge(START, \"generation\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "rag = rag_graph.compile(checkpointer=memory)\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "result = rag.invoke({\"question\": \"Can you show a picture of the brain?\"}, config)\n",
    "result['images']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HtnaxSHqxvmd"
   },
   "source": [
    "## Final Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "HeFLSeEVswLH",
    "outputId": "0bd4f79e-697c-40be-ed08-33c93e0030fb"
   },
   "outputs": [],
   "source": [
    "class ParentGraph(TypedDict):\n",
    "  \"\"\"Represents the state of our graph.\"\"\"\n",
    "  question: str\n",
    "  question_type: str\n",
    "  answer: str\n",
    "  images: List[str]\n",
    "\n",
    "class QueryOutput(TypedDict):\n",
    "    \"\"\"Generated question type.\"\"\"\n",
    "    question_type: Annotated[str, ..., \"Syntactically valid question type.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "rbFt1E1StQTd",
    "outputId": "935ae698-9f25-406f-a0a0-467271ae5cb6"
   },
   "outputs": [],
   "source": [
    "# Define the parent graph\n",
    "parent_graph = StateGraph(ParentGraph)\n",
    "\n",
    "parent_graph.add_node(\"classify_question\", classify_question)\n",
    "parent_graph.add_node(\"sql_subgraph\", sql_subgraph)\n",
    "parent_graph.add_node(\"rag_subgraph\", rag_subgraph)\n",
    "\n",
    "# Add conditional edges based on the routing function\n",
    "parent_graph.add_conditional_edges(\"classify_question\", route_based_on_question_type)\n",
    "\n",
    "parent_graph.add_edge(START, \"classify_question\")\n",
    "\n",
    "# memory = MemorySaver()\n",
    "# parent = parent_graph.compile(checkpointer=memory)\n",
    "# config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "parent = parent_graph.compile()\n",
    "\n",
    "result = parent.invoke({\"question\": \"How many people are there?\"})\n",
    "# result = parent.invoke({\"question\": \"Show me a picture of the brain\"})\n",
    "\n",
    "# print(\"Final Answer:\", result[\"answer\"])\n",
    "print(\"Final Answer:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 778
    },
    "id": "GIO0CVbVwu0b",
    "outputId": "572b3b27-9612-4192-e5a7-b85f1e26c007"
   },
   "outputs": [],
   "source": [
    "display(Image(parent.get_graph(xray=True).draw_mermaid_png(), width=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frcKES2iTGqZ"
   },
   "source": [
    "## Gradio UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XRAv7zebzHR2",
    "outputId": "b6b46ec5-d8b2-420a-bee5-32dbb6a712bc"
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import base64\n",
    "import os\n",
    "\n",
    "def process_question(user_input):\n",
    "  result = parent.invoke({\"question\": user_input})\n",
    "  answer_text = result[\"answer\"]\n",
    "  images = result.get(\"images\")\n",
    "\n",
    "  temp_image_paths = []\n",
    "\n",
    "  if (\n",
    "    images\n",
    "    and len(images) > 0 # Cannot be empty\n",
    "    and images[0].startswith('/9j/4AAQ') # Must be base64\n",
    "    and images[0] != 'No diagrams' # Or we can use this to solve everything\n",
    "  ):\n",
    "    for idx, img_data in enumerate(images):\n",
    "        img_data = img_data.split(',')[1] if ',' in img_data else img_data\n",
    "        img_bytes = base64.b64decode(img_data)\n",
    "        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".jpg\")\n",
    "        temp_file.write(img_bytes)\n",
    "        temp_file.close()\n",
    "        temp_image_paths.append(temp_file.name)\n",
    "\n",
    "  return answer_text, temp_image_paths\n",
    "\n",
    "# process_question(\"Show me a picture of the brain\")\n",
    "# process_question(\"How many patients are there\")\n",
    "# process_question(\"Can you give me a breakdown of the number of people with the different type of cancers?\")\n",
    "# process_question(\"How many patients are there with no tumors detected?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "id": "E9yU_7tZkRFj",
    "outputId": "96e2dbfa-c118-40b9-baab-ee9b195348ed"
   },
   "outputs": [],
   "source": [
    "# Create a Gradio interface with a custom layout\n",
    "\n",
    "banner_url = \"https://i.imgur.com/jn2wz20.png\"\n",
    "\n",
    "with gr.Blocks() as iface:\n",
    "    # Add the banner image\n",
    "    gr.Markdown(f\"\"\"\n",
    "    <div style=\"text-align: center; margin-bottom: 20px;\">\n",
    "        <img src=\"{banner_url}\" alt=\"Doctor Banner\" style=\"max-width: 100%; height: auto;\">\n",
    "    </div>\n",
    "    \"\"\")\n",
    "\n",
    "    # Add the title and description\n",
    "    gr.Markdown(\"\"\"\n",
    "    ## Doctor's Question & Answering System\n",
    "    Enter a question, and the system will classify and process it accordingly. If applicable, images will also be displayed.\n",
    "    \"\"\")\n",
    "\n",
    "    # Add the input and outputs\n",
    "    with gr.Row():\n",
    "        question_input = gr.Textbox(\n",
    "            label=\"Ask Your Question\", lines=2, placeholder=\"Enter your question here...\"\n",
    "        )\n",
    "    with gr.Row():\n",
    "        answer_output = gr.Textbox(label=\"Answer\", lines=8)\n",
    "        images_output = gr.Gallery(label=\"Images\", show_label=True)\n",
    "\n",
    "    # Add the submit button\n",
    "    submit_btn = gr.Button(\"Submit\")\n",
    "    submit_btn.click(\n",
    "        fn=process_question,\n",
    "        inputs=question_input,\n",
    "        outputs=[answer_output, images_output]\n",
    "    )\n",
    "\n",
    "# Launch the Gradio app\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "x29JaCDpXJIp",
    "AFmu0yKLwI_f",
    "PTfOrZmH9c5n",
    "WwRhDytBdAFn",
    "fl8eQ8oMXByN",
    "cDdochbcioiS",
    "w2-8jr_v20ey"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
