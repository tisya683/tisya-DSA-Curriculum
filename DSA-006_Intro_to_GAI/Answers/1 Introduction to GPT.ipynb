{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9513b82c",
   "metadata": {},
   "source": [
    "# Introduction to GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd7ba4a",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "1. Setting up the required variables to call the endpoint GrabGPT API\n",
    "2. Making chat compltion calls to library\n",
    "3. Handling intermittent network and rate limit issues gracefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80164b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install dotenv\n",
    "# %pip install langchain_openai\n",
    "# %pip install langchain_core.messages\n",
    "# %pip install langchain_core.prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fee8994-ca19-4fa0-b586-c8b94f4af0de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03fa41e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure environment variables are loaded before accessing them\n",
    "load_dotenv('test.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acbce11",
   "metadata": {},
   "source": [
    "### Set the API Key environment\n",
    "- gpt-4\n",
    "- gpt-3.5-turbo\n",
    "- langsmith\n",
    "- Additional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3db16bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "gpt = ChatOpenAI(model='gpt-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddf6cd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "gpt3 = ChatOpenAI(model='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4ce8a9",
   "metadata": {},
   "source": [
    "### Introduction to LangSmith\n",
    "\n",
    "LangSmith is a tool in the Langchain ecosystem designed to help monitor, evaluate, and debug language models more effectively. It provides the infrastructure for logging interactions, running tests, and tracking metrics to ensure optimal model performance.\n",
    "\n",
    "We will be covering this topic in depth in future sessions!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e187e4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last step: \n",
    "tracing = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "langsmith = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# Let's you trace everything that is going on in this codespace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0847e38",
   "metadata": {},
   "source": [
    "# What is GPT?\n",
    "\n",
    "GPT refers to a suite of powerful language models developed by OpenAI, such as **GPT-3**, **GPT-4**, and **GPT-4 with Vision**. These models can perform a wide variety of tasks, including text generation, conversation, translation, and image processing (in the case of GPT-4 with Vision).\n",
    "\n",
    "Through OpenAI's API, users can access these models directly via OpenAI’s platform, bypassing the need for third-party integrations like Azure. With an **OpenAI API key**, you have direct access to the latest models OpenAI offers.\n",
    "\n",
    "While our focus for this training is on OpenAI’s models, which are the most widely used and well-documented, the principles for interacting with other large language models (LLMs) like Meta’s LLaMA or Anthropic’s Claude are largely similar.\n",
    "\n",
    "## Available Models through OpenAI API:\n",
    "\n",
    "| Model Name                      | Description                               |\n",
    "|----------------------------------|-------------------------------------------|\n",
    "| **gpt-3.5-turbo**                | A highly efficient variant of GPT-3.5, great for most conversational tasks. |\n",
    "| **gpt-4**                        | The latest iteration of OpenAI’s powerful GPT series, offering enhanced understanding and reasoning abilities. |\n",
    "| **gpt-4-32k**                    | A larger version of GPT-4 with the ability to process longer context windows. |\n",
    "| **gpt-4 with Vision**            | Allows GPT-4 to process both text and images, useful for tasks that combine visual and textual inputs. |\n",
    "| **text-embedding-ada-002**       | A model specialized for creating embeddings for tasks like text similarity, search, and clustering. |\n",
    "| **Whisper**                      | OpenAI’s speech recognition model for transcribing and translating audio. |\n",
    "| **DALL·E**                       | OpenAI’s image generation model, capable of generating images from textual descriptions. |\n",
    "\n",
    "For a comprehensive list of models and capabilities available through OpenAI’s API, you can refer to the [OpenAI API documentation](https://platform.openai.com/docs).\n",
    "\n",
    "---\n",
    "\n",
    "This version reflects the use of the OpenAI API key for direct access to OpenAI's models, rather than relying on Azure or other providers. Let me know if you'd like further adjustments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69a4ed35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='我爱冰淇淋，老干妈和约翰·塞纳。' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 30, 'total_tokens': 57, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-773cae86-b9c0-4bd1-adb8-bf536f5cc3bc-0' usage_metadata={'input_tokens': 30, 'output_tokens': 27, 'total_tokens': 57, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}\n",
      "我爱冰淇淋，老干妈和约翰·塞纳。\n"
     ]
    }
   ],
   "source": [
    "# gpt 4\n",
    "# Run once can le!\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following from English into chinese\"),\n",
    "    HumanMessage(content=\"I love ice cream, lao gan ma and john cena\"),\n",
    "]\n",
    "\n",
    "message = gpt.invoke(messages)\n",
    "print(message)\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dc383b",
   "metadata": {},
   "source": [
    "### Tokens\n",
    "The token limit determines how much information can be handled in a single interaction. For large documents or complex tasks, a higher token limit allows for more extensive input and output, while lower limits mean shorter interactions.\n",
    "\n",
    "Why is this important?\n",
    "The token limit determines how much information can be handled in a single interaction. For large documents or complex tasks, a higher token limit allows for more extensive input and output, while lower limits mean shorter interactions.\n",
    "\n",
    "GPT-4 (8k context model): This version has a maximum token limit of 8,192 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "401f78dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ca539ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='我喜欢吃冰淇淋，老干妈和约翰·塞纳。', response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 30, 'total_tokens': 61, 'prompt_tokens_details': {'cached_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-6057e3bc-4052-4bea-8187-1827adc3feb4-0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gpt 3.5 turbo\n",
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following from English into chinese\"),\n",
    "    HumanMessage(content=\"I love ice cream, lao gan ma and john cena\"),\n",
    "]\n",
    "\n",
    "gpt3.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1643b88d",
   "metadata": {},
   "source": [
    "# Try it out yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "11873c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='A traditional Singaporean breakfast might include Kaya Toast with soft-boiled eggs and a cup of Kopi or Teh (coffee or tea). Kaya Toast is a toasted bread filled with butter and Kaya, a jam made from eggs, sugar, coconut milk, and pandan leaves. The soft-boiled eggs are usually served in a small bowl, lightly seasoned with soy sauce and white pepper. \\n\\nAnother popular breakfast dish is Nasi Lemak, a fragrant rice dish cooked in coconut milk and pandan leaf, served with fried anchovies, peanuts, cucumber, a hard-boiled or fried egg, and sambal (a spicy chili sauce). \\n\\nRoti Prata, an Indian-influenced flatbread, is also a common breakfast dish. It is usually served with a side of curry for dipping. \\n\\nFor those who prefer a noodle dish, Mee Siam, which is rice vermicelli noodles served in a spicy, sweet and sour gravy, is also a popular choice.\\n\\nOf course, as Singapore is a multicultural city, you can also find all sorts of international breakfast offerings, from Western-style breakfasts to Chinese dim sum and more.' response_metadata={'token_usage': {'completion_tokens': 237, 'prompt_tokens': 27, 'total_tokens': 264, 'prompt_tokens_details': {'cached_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-a9e8d206-4f59-4f99-80ab-79fe8172ad15-0'\n",
      "A traditional Singaporean breakfast might include Kaya Toast with soft-boiled eggs and a cup of Kopi or Teh (coffee or tea). Kaya Toast is a toasted bread filled with butter and Kaya, a jam made from eggs, sugar, coconut milk, and pandan leaves. The soft-boiled eggs are usually served in a small bowl, lightly seasoned with soy sauce and white pepper. \n",
      "\n",
      "Another popular breakfast dish is Nasi Lemak, a fragrant rice dish cooked in coconut milk and pandan leaf, served with fried anchovies, peanuts, cucumber, a hard-boiled or fried egg, and sambal (a spicy chili sauce). \n",
      "\n",
      "Roti Prata, an Indian-influenced flatbread, is also a common breakfast dish. It is usually served with a side of curry for dipping. \n",
      "\n",
      "For those who prefer a noodle dish, Mee Siam, which is rice vermicelli noodles served in a spicy, sweet and sour gravy, is also a popular choice.\n",
      "\n",
      "Of course, as Singapore is a multicultural city, you can also find all sorts of international breakfast offerings, from Western-style breakfasts to Chinese dim sum and more.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1) Ask chatgpt for Singapore's most popular breakfast meal!\n",
    "2) Take note of the tokens used\n",
    "3) Find out the cost!\n",
    "'''\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a Singaporean foodie expert\"),\n",
    "    HumanMessage(content=\"What is a Singaporean breakfast like?\"),\n",
    "]\n",
    "\n",
    "output = gpt.invoke(messages)\n",
    "print(output)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4369aa80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A traditional Singaporean breakfast often consists of \"kaya toast\", which is toasted bread slathered with coconut jam (kaya) and butter, served with soft-boiled eggs that you crack into a bowl, season with soy sauce and pepper, then dip your toast into. This is typically accompanied by a cup of strong, sweet coffee or tea, locally known as 'kopi' or 'teh'. \n",
      "\n",
      "Alternatively, many Singaporeans also enjoy dishes like \"Roti Prata\" (a South-Indian influenced flatbread served with curry), \"Nasi Lemak\" (rice cooked in coconut milk, served with fried anchovies, peanuts, egg, cucumber slices and chilli paste), or \"Chwee Kueh\" (steamed rice cakes topped with preserved radish and chilli). There's a huge variety of local breakfast dishes due to the multicultural heritage of Singapore. \n",
      "\n",
      "Singaporeans also love their hawker centres, which are open-air food courts offering a variety of inexpensive local food from various cultures, so a breakfast outing could involve sampling a few of these different dishes.\n"
     ]
    }
   ],
   "source": [
    "# Content\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ab72657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completion_tokens': 223, 'prompt_tokens': 27, 'total_tokens': 250, 'prompt_tokens_details': {'cached_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Token usage\n",
    "print(output.response_metadata['token_usage'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253b232f",
   "metadata": {},
   "source": [
    "## Prompt Engineering & Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d9b44198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Prompt from Template-----\n",
      "messages=[HumanMessage(content='Tell me a joke about cats.', additional_kwargs={}, response_metadata={})]\n",
      "Why don't cats play poker in the jungle?\n",
      "\n",
      "Because there are too many cheetahs!\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate # Mimic what you will see when using ChatGPT UI\n",
    "\n",
    "# PART 1: Create a ChatPromptTemplate using a template string\n",
    "print(\"-----Prompt from Template-----\")\n",
    "template = \"Tell me a joke about {topic}.\"\n",
    "prompt_template = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
    "print(prompt)\n",
    "\n",
    "result = gpt.invoke(prompt)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d364f17",
   "metadata": {},
   "source": [
    "#### You will notice there is alot of room for dynamic changes using a prompt template as compared to using the standard chat template above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15d6310d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Prompt with Multiple Placeholders -----\n",
      "\n",
      "Sure, here's a short, funny story about a panda.\n",
      "\n",
      "Once upon a time, in the heart of China's bamboo forests, lived a panda named Paddy. Paddy was not your average panda; he was known for his peculiar love for apple pies.\n",
      "\n",
      "One day, Paddy heard from his bird friend, Chirpy, that the neighboring village was hosting an annual apple pie contest. Without a second thought, Paddy decided to participate. He was quite the spectacle, a panda amongst humans, but he didn't mind. He was there for the pie.\n",
      "\n",
      "On the day of the contest, Paddy was nervous. He had practiced baking with bamboo shoots, but never with apples. As he started baking, he realized he had forgotten the recipe back home. In his panic, he started throwing in whatever he could remember - apples, bamboo (yes, bamboo!), honey, and a dash of luck.\n",
      "\n",
      "Finally, the time came to present the pies. The judges were surprised to see a panda, but they were even more surprised when they tasted Paddy's pie. It was a unique blend of sweet apples and crunchy bamboo, an unexpectedly delicious combination!\n",
      "\n",
      "Paddy won the contest, not only for the taste but for the most original pie. From that day forward, he became a legend in the village, known as the Panda Pie Maker. The villagers even started adding bamboo to their pies, a trend set by a panda with a sweet tooth. And Paddy? He was just happy he had plenty of apple pies to eat!\n",
      "\n",
      "And that's the short, funny story about Paddy, the apple pie-loving panda.\n"
     ]
    }
   ],
   "source": [
    "# PART 2: Prompt with Multiple Placeholders\n",
    "print(\"\\n----- Prompt with Multiple Placeholders -----\\n\")\n",
    "template_multiple = \"\"\"You are a helpful assistant.\n",
    "Human: Tell me a {adjective} short story about a {animal}.\n",
    "Assistant:\"\"\"\n",
    "\n",
    "prompt_multiple = ChatPromptTemplate.from_template(template_multiple)\n",
    "prompt = prompt_multiple.invoke({\"adjective\": \"funny\", \"animal\": \"panda\"})\n",
    "\n",
    "result = gpt.invoke(prompt)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91a8b020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Prompt with System and Human Messages (Tuple) -----\n",
      "\n",
      "1. Why don't lawyers go to the beach? Because cats keep trying to bury them in the sand!\n",
      "\n",
      "2. Why was the lawyer skimming through the bible before he died? He was looking for loopholes!\n",
      "\n",
      "3. How does a lawyer sleep? First they lie on one side, then they lie on the other!\n"
     ]
    }
   ],
   "source": [
    "# PART 3: Prompt with System and Human Messages (Using Tuples)\n",
    "print(\"\\n----- Prompt with System and Human Messages (Tuple) -----\\n\")\n",
    "\n",
    "messages = [\n",
    "    (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
    "    (\"human\", \"Tell me {joke_count} jokes.\"),\n",
    "]\n",
    "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "prompt = prompt_template.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})\n",
    "result = gpt.invoke(prompt)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d938831e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, I'd be happy to help with that. \n",
      "\n",
      "Firstly, we need to find the antiderivative (also known as the integral) of the function 3x^2 - 2x + 1. \n",
      "\n",
      "The antiderivative F(x) of a function f(x) = 3x^2 - 2x + 1 is the function F whose derivative is f. \n",
      "\n",
      "The antiderivative is found using the Power Rule for integrals, which states that the integral of x^n dx is (1/(n+1))x^(n+1), where n ≠ -1.\n",
      "\n",
      "So, let's find the antiderivative of each term in 3x^2 - 2x + 1:\n",
      "\n",
      "∫3x^2 dx = (3/3)x^(2+1) = x^3 \n",
      "∫-2x dx = (-2/2)x^(1+1) = -x^2 \n",
      "∫1 dx = x (since the integral of a constant is just the constant times x)\n",
      "\n",
      "Adding these together gives the antiderivative of the original function:\n",
      "\n",
      "F(x) = x^3 - x^2 + x\n",
      "\n",
      "Next, we evaluate this antiderivative from 0 to 2 to find the definite integral. This is done using the Fundamental Theorem of Calculus, which states that the definite integral from a to b of f(x) dx is F(b) - F(a), where F is an antiderivative of f. \n",
      "\n",
      "F(2) = 2^3 - 2^2 + 2 = 8 - 4 + 2 = 6\n",
      "F(0) = 0^3 - 0^2 + 0 = 0 \n",
      "\n",
      "So, the definite integral from 0 to 2 of 3x^2 - 2x + 1 dx is F(2) - F(0) = 6 - 0 = 6.\n"
     ]
    }
   ],
   "source": [
    "# Try it out yourself\n",
    "\"\"\"\n",
    "Write a Python code snippet using ChatPromptTemplate to:\n",
    "\n",
    "1) Create the system and human messages using tuples.\n",
    "2) The system message should say: \"You are a calculus expert tutor.\"\n",
    "3) The human message should say: \"Help me solve {problem_count} calculus problems.\"\n",
    "4) Use problem_count = \"\\int_0^2 (3x^2 - 2x + 1) \\, dx\" as an input.\n",
    "5) Invoke the prompt and print the response from the model.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    (\"system\", \"You are a calculus expert tutor.\"),\n",
    "    (\"human\", \"Help me solve {problem_count} calculus problems.\"),\n",
    "]\n",
    "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "prompt = prompt_template.invoke({\"problem_count\": \"\\int_0^2 (3x^2 - 2x + 1) \\, dx\"})\n",
    "result = gpt.invoke(prompt)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8133dfbd",
   "metadata": {},
   "source": [
    "### Additional\n",
    "\n",
    "From lazy prompt to detailed prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e05917b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['lazy_prompt', 'task'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'hardkothari', 'lc_hub_repo': 'prompt-maker', 'lc_hub_commit_hash': 'c5db8eeefa7be4862a9599b759608dd10ee53f53910838f69abb5ab31c257c2d'} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert Prompt Writer for Large Language Models.\\n\\n'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['lazy_prompt', 'task'], input_types={}, partial_variables={}, template='Your goal is to improve the prompt given below for {task} :\\n--------------------\\n\\nPrompt: {lazy_prompt}\\n\\n--------------------\\n\\nHere are several tips on writing great prompts:\\n\\n-------\\n\\nStart the prompt by stating that it is an expert in the subject.\\n\\nPut instructions at the beginning of the prompt and use ### or to separate the instruction and context \\n\\nBe specific, descriptive and as detailed as possible about the desired context, outcome, length, format, style, etc \\n\\n---------\\n\\nHere\\'s an example of a great prompt:\\n\\nAs a master YouTube content creator, develop an engaging script that revolves around the theme of \"Exploring Ancient Ruins.\"\\n\\nYour script should encompass exciting discoveries, historical insights, and a sense of adventure.\\n\\nInclude a mix of on-screen narration, engaging visuals, and possibly interactions with co-hosts or experts.\\n\\nThe script should ideally result in a video of around 10-15 minutes, providing viewers with a captivating journey through the secrets of the past.\\n\\nExample:\\n\\n\"Welcome back, fellow history enthusiasts, to our channel! Today, we embark on a thrilling expedition...\"\\n\\n-----\\n\\nNow, improve the prompt.\\n\\nIMPROVED PROMPT:'), additional_kwargs={})]\n",
      "['lazy_prompt', 'task']\n"
     ]
    }
   ],
   "source": [
    "# We can take prompts that were pre made by people!\n",
    "from langchain import hub\n",
    "prompt = hub.pull(\"hardkothari/prompt-maker\")\n",
    "\n",
    "print(prompt)\n",
    "print(prompt.input_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ec3c9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are an expert Prompt Writer for Large Language Models.\\n\\n', additional_kwargs={}, response_metadata={}), HumanMessage(content='Your goal is to improve the prompt given below for Review the Singaporean dish Laksa and Kaya Toast :\\n--------------------\\n\\nPrompt: You are an avid foodie reviewer who loves to give detailed review about the food\\n\\n--------------------\\n\\nHere are several tips on writing great prompts:\\n\\n-------\\n\\nStart the prompt by stating that it is an expert in the subject.\\n\\nPut instructions at the beginning of the prompt and use ### or to separate the instruction and context \\n\\nBe specific, descriptive and as detailed as possible about the desired context, outcome, length, format, style, etc \\n\\n---------\\n\\nHere\\'s an example of a great prompt:\\n\\nAs a master YouTube content creator, develop an engaging script that revolves around the theme of \"Exploring Ancient Ruins.\"\\n\\nYour script should encompass exciting discoveries, historical insights, and a sense of adventure.\\n\\nInclude a mix of on-screen narration, engaging visuals, and possibly interactions with co-hosts or experts.\\n\\nThe script should ideally result in a video of around 10-15 minutes, providing viewers with a captivating journey through the secrets of the past.\\n\\nExample:\\n\\n\"Welcome back, fellow history enthusiasts, to our channel! Today, we embark on a thrilling expedition...\"\\n\\n-----\\n\\nNow, improve the prompt.\\n\\nIMPROVED PROMPT:', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "new_prompt = prompt.invoke(\n",
    "    {\n",
    "        \"lazy_prompt\": \"You are an avid foodie reviewer who loves to give detailed review about the food\", \n",
    "        \"task\": \"Review the Singaporean dish Laksa and Kaya Toast\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(new_prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02f625f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='### As an expert food critic renowned for your meticulous reviews and profound culinary insights, craft a captivating and detailed review of two iconic Singaporean dishes: Laksa and Kaya Toast.\\n\\n### Your review should delve into the rich history and cultural significance of these dishes, exploring the intricate flavors and textures that make them beloved staples in Singaporean cuisine. \\n\\n### Provide a sensory experience for your readers by vividly describing the aroma, taste, and presentation of both Laksa and Kaya Toast. \\n\\n### Share your thoughts on the authenticity of the preparation, the quality of ingredients, and any unique twists or variations that set these dishes apart.\\n\\n### Your review should be both informative and engaging, offering readers a glimpse into the culinary delights of Singapore through your expert perspective.\\n\\n### Aim for a review length of around 600-800 words, ensuring that every detail is carefully considered and eloquently articulated to captivate your audience and leave them craving a taste of Singapore.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 191, 'prompt_tokens': 270, 'total_tokens': 461, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-b5e14bb1-8190-41d8-8ec1-795ffee9c46b-0' usage_metadata={'input_tokens': 270, 'output_tokens': 191, 'total_tokens': 461, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "result = gpt3.invoke(new_prompt)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc90f635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### As an expert food critic renowned for your meticulous reviews and profound culinary insights, craft a captivating and detailed review of two iconic Singaporean dishes: Laksa and Kaya Toast.\\n\\n### Your review should delve into the rich history and cultural significance of these dishes, exploring the intricate flavors and textures that make them beloved staples in Singaporean cuisine. \\n\\n### Provide a sensory experience for your readers by vividly describing the aroma, taste, and presentation of both Laksa and Kaya Toast. \\n\\n### Share your thoughts on the authenticity of the preparation, the quality of ingredients, and any unique twists or variations that set these dishes apart.\\n\\n### Your review should be both informative and engaging, offering readers a glimpse into the culinary delights of Singapore through your expert perspective.\\n\\n### Aim for a review length of around 600-800 words, ensuring that every detail is carefully considered and eloquently articulated to captivate your audience and leave them craving a taste of Singapore.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d55772",
   "metadata": {},
   "source": [
    "## Few-Shot Prompt Template Example\n",
    "In this section, we'll learn how to create a simple prompt template that helps guide the model by providing example inputs and outputs. This technique, known as few-shotting, involves showing the model a few examples to help it understand the task better. It is a powerful way to improve the quality of the generated output, especially when the task is complex or context-dependent.\n",
    "\n",
    "A few-shot prompt template can be built from a fixed set of examples or can be dynamically constructed using an Example Selector class, which is responsible for selecting relevant examples from a pre-defined set based on the query.\n",
    "\n",
    "## Parameter Explanations\n",
    "\n",
    "#### Prompt Template: A framework that structures your prompt and integrates a set of few-shot examples to guide the model's behavior.\n",
    "\n",
    "#### Few-Shotting: Providing a series of example inputs and outputs to the model in the prompt. This helps the model generate better responses by mimicking the patterns in the examples.\n",
    "\n",
    "If you want to read more about [Few-Shot-Prompting Papers](https://arxiv.org/abs/2005.14165)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a49079",
   "metadata": {},
   "source": [
    "## Difference Between Zero-Shot, One-Shot, and Few-Shot Learning\n",
    "\n",
    "- **Zero-Shot Learning**: In zero-shot learning, the model is able to perform a task without having seen any examples or prior data for that specific task. It relies on knowledge transfer from other tasks or context provided by the model.\n",
    "\n",
    "- **One-Shot Learning**: In one-shot learning, the model is trained on only one example of each task or class and is expected to generalize well enough to perform accurately on new, unseen data.\n",
    "\n",
    "- **Few-Shot Learning**: In few-shot learning, the model is trained on a small number of examples (usually a handful) for each class or task, and it uses this limited data to make predictions or perform the task on new examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bc582d",
   "metadata": {},
   "source": [
    "# Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fbce165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence is positive.\n"
     ]
    }
   ],
   "source": [
    "# Zero-shot example: Sentiment analysis (classification task with no prior examples)\n",
    "\n",
    "# System message defines the assistant's role\n",
    "system_message = SystemMessage(content=\"You are a helpful assistant. Who is great at picking up the nuances in a sentence and direct in the way you respond\")\n",
    "\n",
    "# Human message asks the model to classify the sentiment of the sentence\n",
    "human_message = HumanMessage(content=\"Classify the following sentence as positive, negative, or neutral: 'The product quality is excellent and I love it!'\")\n",
    "\n",
    "# Send the conversation to the model\n",
    "response = gpt([system_message, human_message])\n",
    "\n",
    "# Output the model's response\n",
    "print(response.content.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3740ad48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence is neutral. It contains both positive and negative aspects of the product.\n"
     ]
    }
   ],
   "source": [
    "system_message = SystemMessage(content=\"You are a helpful assistant. Who is great at picking up the nuances in a sentence and direct in the way you respond.\")\n",
    "human_message = HumanMessage(\n",
    "    content= \"Classify the following sentence as positive, negative, or neutral:\\\n",
    "    'The product works well most of the time, but there are moments when it suddenly stops working,\\\n",
    "    which can be frustrating. However, I think it's a decent option overall, and the design is nice,\\\n",
    "    though I expected a bit more durability for the price.'\"\n",
    ")\n",
    "\n",
    "response = gpt([system_message, human_message])\n",
    "\n",
    "print(response.content.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374eeda3",
   "metadata": {},
   "source": [
    "# One shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0923631f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Data is the new oil.' => 'Les données sont le nouveau pétrole.'\n"
     ]
    }
   ],
   "source": [
    "# System message defines the assistant's role\n",
    "system_message = SystemMessage(content=\"You are a helpful assistant.\")\n",
    "\n",
    "# Human message provides a one-shot example of English-to-French translation and asks for a new translation\n",
    "human_message = HumanMessage(content=(\n",
    "    \"Translate the following sentence from English to French:\\n\"\n",
    "    \"Example: 'I love data science.' => 'J'adore la science des données.'\\n\"\n",
    "    \"Now translate: 'Data is the new oil.'\"\n",
    "))\n",
    "\n",
    "# Send the conversation to the model\n",
    "response = gpt3([system_message, human_message])\n",
    "\n",
    "# Output the model's response\n",
    "print(response.content.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968f8e17-7b59-48ec-8543-bae5a15a3899",
   "metadata": {},
   "source": [
    "## Few Shots\n",
    "\n",
    "One of the most effective ways to improve model performance is to give a model examples of what you want it to do. The technique of adding example inputs and expected outputs to a model prompt is known as \"few-shot prompting\". The technique is based on the Language Models are Few-Shot Learners paper. There are a few things to think about when doing few-shot prompting:\n",
    "\n",
    "How are examples generated?\n",
    "How many examples are in each prompt?\n",
    "How are examples selected at runtime?\n",
    "How are examples formatted in the prompt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fa21146b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import(\n",
    "    ChatPromptTemplate,\n",
    "    FewShotChatMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f7e466e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can create some examples to show our AI what we want\n",
    "examples = [\n",
    "    {\"input\":\"2+2\", \"output\": \"4\"},\n",
    "    {\"input\":\"2+3\", \"output\": \"5\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e642aa2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: 2+2\n",
      "AI: 4\n",
      "Human: 2+3\n",
      "AI: 5\n"
     ]
    }
   ],
   "source": [
    "#Create an example_prompt\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt = example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "print(few_shot_prompt.format())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "21b35de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('name', None)\n",
      "('examples', [{'input': '2+2', 'output': '4'}, {'input': '2+3', 'output': '5'}])\n",
      "('example_selector', None)\n",
      "('input_variables', [])\n",
      "('optional_variables', [])\n",
      "('input_types', {})\n",
      "('output_parser', None)\n",
      "('partial_variables', {})\n",
      "('metadata', None)\n",
      "('tags', None)\n",
      "('example_prompt', ChatPromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], input_types={}, partial_variables={}, template='{output}'), additional_kwargs={})]))\n"
     ]
    }
   ],
   "source": [
    "for item in few_shot_prompt:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6fb71dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message=\"You are a wonderous wizard of math.\"\n",
    "human_template=\"{input}\"\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_message)\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        system_message_prompt,\n",
    "        few_shot_prompt,\n",
    "        human_message_prompt,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d18877bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The square of 9 is 81.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 52, 'total_tokens': 61, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-3c3db7bd-92be-4a6b-a30d-ec62360c34bb-0' usage_metadata={'input_tokens': 52, 'output_tokens': 9, 'total_tokens': 61, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# We will use chain here each item is called a runnable, \n",
    "# we will dive deeper in the future session\n",
    "chain = final_prompt | gpt\n",
    "\n",
    "results = chain.invoke(\"What's the square of 9?\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06e580f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\",\n",
    "        \"answer\": \"\"\"\n",
    "            Are follow up questions needed here: Yes.\n",
    "            Follow up: How old was Muhammad Ali when he died?\n",
    "            Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
    "            Follow up: How old was Alan Turing when he died?\n",
    "            Intermediate answer: Alan Turing was 41 years old when he died.\n",
    "            So the final answer is: Muhammad Ali\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"When was the founder of craigslist born?\",\n",
    "        \"answer\": \"\"\"\n",
    "            Are follow up questions needed here: Yes.\n",
    "            Follow up: Who was the founder of craigslist?\n",
    "            Intermediate answer: Craigslist was founded by Craig Newmark.\n",
    "            Follow up: When was Craig Newmark born?\n",
    "            Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
    "            So the final answer is: December 6, 1952\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Who was the maternal grandfather of George Washington?\",\n",
    "        \"answer\": \"\"\"\n",
    "            Are follow up questions needed here: Yes.\n",
    "            Follow up: Who was the mother of George Washington?\n",
    "            Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
    "            Follow up: Who was the father of Mary Ball Washington?\n",
    "            Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
    "            So the final answer is: Joseph Ball                                     \n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Are both the directors of Jaws and Casino Royale from the same country?\",\n",
    "        \"answer\": \"\"\"\n",
    "            Are follow up questions needed here: Yes.\n",
    "            Follow up: Who is the director of Jaws?\n",
    "            Intermediate Answer: The director of Jaws is Steven Spielberg.\n",
    "            Follow up: Where is Steven Spielberg from?\n",
    "            Intermediate Answer: The United States.\n",
    "            Follow up: Who is the director of Casino Royale?\n",
    "            Intermediate Answer: The director of Casino Royale is Martin Campbell.\n",
    "            Follow up: Where is Martin Campbell from?\n",
    "            Intermediate Answer: New Zealand.\n",
    "            So the final answer is: No\n",
    "        \"\"\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f1b43404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Define the structure for individual examples\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"answer\"],\n",
    "    template=\"Question: {question}\\nAnswer: {answer}\\n\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f0f99cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='Question: Who lived longer, Muhammad Ali or Alan Turing?\\nAnswer: \\nAre follow up questions needed here: Yes.\\nFollow up: How old was Muhammad Ali when he died?\\nIntermediate answer: Muhammad Ali was 74 years old when he died.\\nFollow up: How old was Alan Turing when he died?\\nIntermediate answer: Alan Turing was 41 years old when he died.\\nSo the final answer is: Muhammad Ali\\n\\n'\n",
      "Question: Who lived longer, Muhammad Ali or Alan Turing?\n",
      "Answer: \n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: How old was Muhammad Ali when he died?\n",
      "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
      "Follow up: How old was Alan Turing when he died?\n",
      "Intermediate answer: Alan Turing was 41 years old when he died.\n",
      "So the final answer is: Muhammad Ali\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Invoke the example prompt with the first example\n",
    "print(example_prompt.invoke(examples[0]))\n",
    "print(example_prompt.invoke(examples[0]).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "33fd84a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who lived longer, Muhammad Ali or Alan Turing?\n",
      "Answer: \n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: How old was Muhammad Ali when he died?\n",
      "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
      "Follow up: How old was Alan Turing when he died?\n",
      "Intermediate answer: Alan Turing was 41 years old when he died.\n",
      "So the final answer is: Muhammad Ali\n",
      "\n",
      "\n",
      "\n",
      "Question: When was the founder of craigslist born?\n",
      "Answer: \n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who was the founder of craigslist?\n",
      "Intermediate answer: Craigslist was founded by Craig Newmark.\n",
      "Follow up: When was Craig Newmark born?\n",
      "Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
      "So the final answer is: December 6, 1952\n",
      "\n",
      "\n",
      "\n",
      "Question: Who was the maternal grandfather of George Washington?\n",
      "Answer: \n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who was the mother of George Washington?\n",
      "Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
      "Follow up: Who was the father of Mary Ball Washington?\n",
      "Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
      "So the final answer is: Joseph Ball\n",
      "\n",
      "\n",
      "\n",
      "Question: Are both the directors of Jaws and Casino Royale from the same country?\n",
      "Answer: \n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who is the director of Jaws?\n",
      "Intermediate Answer: The director of Jaws is Steven Spielberg.\n",
      "Follow up: Where is Steven Spielberg from?\n",
      "Intermediate Answer: The United States.\n",
      "Follow up: Who is the director of Casino Royale?\n",
      "Intermediate Answer: The director of Casino Royale is Martin Campbell.\n",
      "Follow up: Where is Martin Campbell from?\n",
      "Intermediate Answer: New Zealand.\n",
      "So the final answer is: No\n",
      "\n",
      "\n",
      "\n",
      "Question: Who was the father of Mary Ball Washington?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import FewShotPromptTemplate\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")\n",
    "\n",
    "print(\n",
    "    prompt.invoke({\"input\": \"Who was the father of Mary Ball Washington?\"}).to_string()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "john_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
