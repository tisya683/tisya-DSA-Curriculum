{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtBgN8DmJ9ir"
      },
      "source": [
        "# Introduction to LangChain\n",
        "\n",
        "## What is LangChain?\n",
        "LangChain is a framework that helps you combine **language models** with various tools, APIs, and logic to create dynamic workflows and applications. It allows you to build **structured workflows** (LangGraphs) or flexible, real-time decision-makers (Agents) using large language models like GPT.\n",
        "\n",
        "You can think of LangChain as a **bridge/chain** that connects language models to different tools, databases, or functions, enabling the model to perform more complex tasks.\n",
        "\n",
        "---\n",
        "\n",
        "## Example\n",
        "Let’s say you want to build a **flight booking assistant** using LangChain. You can use a **LangGraph** (component of LangChain) to structure the assistant's tasks like this:\n",
        "1. Ask for the user’s destination.\n",
        "2. Search for available flights using an API.\n",
        "3. Show the user the available flights.\n",
        "4. Confirm the booking.\n",
        "\n",
        "## Analogy:\n",
        "Imagine LangChain as a GPS navigation system for your language models:\n",
        "\n",
        "If you follow a predefined route (LangGraph), it guides you step-by-step to your destination.\n",
        "But if there’s a detour or you decide to explore along the way, you can use an Agent to decide the best route dynamically, based on what’s happening in real-time.\n",
        "\n",
        "**Refer to the slides for the workflow!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQqbi0GfJ9it"
      },
      "source": [
        "## SetUp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4FPgShzfad5",
        "outputId": "5c9b2338-aa89-48b0-c12c-463941fd46db"
      },
      "outputs": [],
      "source": [
        "# Download necessary\n",
        "%pip install FastAPI langserve sse_starlette wikipedia python-dotenv langchain-openai langchain-core langchain langchain-community google-search-results langgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oGqX5ZeVJ9iu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFg80zZrJ9iv",
        "outputId": "4b3935c9-5e0d-4d50-f3f6-c2d2eda55da2"
      },
      "outputs": [],
      "source": [
        "load_dotenv('template.env')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "x7qwQbf5J9iv"
      },
      "outputs": [],
      "source": [
        "# Call in the model\n",
        "# 'gpt-4'\n",
        "# 'gpt-3.5-turbo'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JTmixPh2J9iv"
      },
      "outputs": [],
      "source": [
        "# Track out cost and understand how langchain works\n",
        "tracing = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
        "langsmith = os.getenv(\"LANGCHAIN_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnhZAPlAVJgU",
        "outputId": "c18ab8b2-6fc3-448b-d4af-c4719409f62b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QmHKF1GJ9iv"
      },
      "source": [
        "## LangChain Example (1)\n",
        "Let's go back to the slides"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Y3QYeIvtPQAi"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import(\n",
        "    ChatPromptTemplate,\n",
        "    FewShotChatMessagePromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate\n",
        ")\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.chat_models import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fDV4J6HZPP6b"
      },
      "outputs": [],
      "source": [
        "# Define the prompt template with placeholders for genre and number of recommendations\n",
        "\n",
        "# \"system\", \"You are a movie critic who recommends movies based on genre.\"\n",
        "# \"human\", \"Recommend {movie_count} movies in the {genre} genre.\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a movie critic who recommends movies based on genre.\"),\n",
        "        (\"human\", \"Recommend {movie_count} movies in the {genre} genre.\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1X-B5QBrQhLC"
      },
      "outputs": [],
      "source": [
        "# Generate our few shot examples from a famous movie critic that you respect their opinion\n",
        "examples = [\n",
        "    {\n",
        "        \"input\": (\"human\", \"Recommend 3 movies in the science fiction genre.\"),\n",
        "        \"output\": (\"system\", \"Here are 3 must-watch science fiction movies:\\n\"\n",
        "                    \"1. Blade Runner 2049 (2017) – A visually stunning sequel that explores themes of identity and artificial intelligence.\\n\"\n",
        "                    \"2. The Matrix (1999) – A groundbreaking film that blends philosophy, action, and technology in a dystopian future.\\n\"\n",
        "                    \"3. Interstellar (2014) – A space epic that delves into the emotional and scientific challenges of interstellar travel.\")\n",
        "    },\n",
        "    {\n",
        "        \"input\": (\"human\", \"Recommend 2 movies in the horror genre.\"),\n",
        "        \"output\": (\"system\", \"Here are 2 terrifying horror movies:\\n\"\n",
        "                    \"1. Hereditary (2018) – A chilling psychological horror film that explores family secrets and supernatural elements.\\n\"\n",
        "                    \"2. The Conjuring (2013) – Based on true events, this film is a masterclass in building suspense and delivering scares.\")\n",
        "    },\n",
        "    {\n",
        "        \"input\": (\"human\", \"Recommend 4 movies in the action genre.\"),\n",
        "        \"output\": (\"system\", \"Here are 4 adrenaline-pumping action movies:\\n\"\n",
        "                    \"1. Mad Max: Fury Road (2015) – A high-octane chase across a post-apocalyptic desert with stunning visuals and practical effects.\\n\"\n",
        "                    \"2. John Wick (2014) – A sleek and stylish film featuring expertly choreographed fight sequences.\\n\"\n",
        "                    \"3. Die Hard (1988) – The ultimate action movie with a thrilling story of a cop fighting terrorists in a high-rise building.\\n\"\n",
        "                    \"4. The Dark Knight (2008) – A superhero film that combines action with a deep exploration of morality and chaos.\")\n",
        "    },\n",
        "    {\n",
        "        \"input\": (\"human\", \"Recommend 5 movies in the romantic comedy genre.\"),\n",
        "        \"output\": (\"system\", \"Here are 5 delightful romantic comedies:\\n\"\n",
        "                    \"1. Notting Hill (1999) – A charming story of an ordinary bookstore owner falling for a famous actress.\\n\"\n",
        "                    \"2. 10 Things I Hate About You (1999) – A modern retelling of Shakespeare’s *The Taming of the Shrew*, set in high school.\\n\"\n",
        "                    \"3. Crazy Rich Asians (2018) – A romantic comedy that blends cultural identity with lavish settings and heartwarming moments.\\n\"\n",
        "                    \"4. When Harry Met Sally (1989) – A classic film exploring whether men and women can ever just be friends.\\n\"\n",
        "                    \"5. The Proposal (2009) – A hilarious and heartwarming movie about a fake engagement that turns into real love.\")\n",
        "    }\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QsfW0GT1QRd3"
      },
      "outputs": [],
      "source": [
        "# For us to tell llm what kind of task and query the llm will receive\n",
        "example_prompt \n",
        "\n",
        "# Past examples of a good reply\n",
        "few_shot_prompt "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XhGvnL7nQTsm"
      },
      "outputs": [],
      "source": [
        "# The final prompt we will use to prompt things from the LLM\n",
        "final_prompt "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZG8QoIeTAz3",
        "outputId": "49405e60-7243-477c-93e4-e38cfe0021c8"
      },
      "outputs": [],
      "source": [
        "# Check that everything is working well\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hrzijSXaPPyX"
      },
      "outputs": [],
      "source": [
        "# Create the combined chain using LangChain Expression Language (LCEL) each of this block is a runnable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BJpPRBgP_7H",
        "outputId": "02cf855c-ee3f-457a-9320-946b38174f1a"
      },
      "outputs": [],
      "source": [
        "# Run the chain with parameters for genre and number of recommendations\n",
        "\n",
        "\n",
        "# Output\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzNV6l4YJ9iw"
      },
      "source": [
        "### You are probably wondering what is StrOutputParser() ??\n",
        "Well, it's a great way to force our output to be formatted a certain way!  \n",
        "There are many outputs you can form using OutputParsers in LangChain.\n",
        "\n",
        "The `StrOutputParser()` is just one of many types of parsers. It converts the output from the language model into a simple string format, which is useful when you want to ensure the output is clean and easy to handle.\n",
        "\n",
        "---\n",
        "\n",
        "### Types of OutputParser you can use:\n",
        "LangChain provides several types of output parsers that you can use depending on the format you need for the task. Here's a list of some common ones:\n",
        "\n",
        "1. **StrOutputParser**: Parses the output as a plain string.\n",
        "2. **JsonOutputParser**: Ensures that the output is formatted as valid JSON.\n",
        "3. **RegexOutputParser**: Uses regular expressions to parse specific parts of the output.\n",
        "4. **BooleanOutputParser**: Converts the output into a boolean (True/False) based on the content.\n",
        "5. **ListOutputParser**: Formats the output as a list of items.\n",
        "6. **CsvOutputParser**: Parses the output into CSV format.\n",
        "7. **DictOutputParser**: Parses the output as a Python dictionary.\n",
        "\n",
        "For more detailed information about **OutputParser** and how to use them, you can check out the official [LangChain OutputParser documentation](https://docs.langchain.com/docs/modules/chains/prompts/output_parsers).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HJ-2awm8X9IG"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from pydantic import BaseModel, Field, field_validator\n",
        "from typing import List, Optional\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "2i1DkqMuepuS"
      },
      "outputs": [],
      "source": [
        "# Now creating joke db for example we want the question and answer to the joke in the db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "g-6VY0t8xeFJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ksaENV67xhTo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Z5-v7ZTnxvYk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FprJFFAKx5QZ",
        "outputId": "534882fa-1963-4ac6-de6a-4734c1f0c4d5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDYG73RpJ9iw"
      },
      "source": [
        "## Intermediate Langchain\n",
        "\n",
        "### Feedback provider"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "HGWWxqRJzb9t"
      },
      "outputs": [],
      "source": [
        "from langchain.schema.runnable import RunnableBranch, RunnableLambda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Uses of RunnableBranch:\n",
        "\n",
        "### Conditional Logic: It allows you to create conditional branches in your workflows where different actions are taken depending on the input. It’s like an “if-else” statement in programming but applied to language models and workflows.\n",
        "\n",
        "### Dynamic Control: Based on the input data or the result of a previous step, it can decide which branch of logic to follow, making workflows more dynamic and adaptable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "HDhgYG9HyF1p"
      },
      "outputs": [],
      "source": [
        "# Define prompt templates for different feedback types\n",
        "positive_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Generate a thank you note for this positive feedback: {feedback}.\"\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "negative_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\n",
        "            \"human\",\n",
        "          \"Generate a response addressing this negative feedback: {feedback}.\"\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "neutral_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Generate a request for more details for this neutral feedback: {feedback}.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "escalate_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Generate a message to escalate this feedback to a human agent: {feedback}.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define the feedback classification template\n",
        "classification_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Classify the sentiment of this feedback as positive, negative, neutral, or escalate: {feedback}.\"\n",
        "        ),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEj-gou1J9iw",
        "outputId": "7fed5d5e-b4a3-4621-c629-2591aee76ef6"
      },
      "outputs": [],
      "source": [
        "# Define the runnable branches for handling feedback\n",
        "branches = RunnableBranch(\n",
        "\n",
        ")\n",
        "\n",
        "# To force our response to take this actions!\n",
        "uppercase_output\n",
        "count_words\n",
        "\n",
        "# Create the classification chain to classify \n",
        "\n",
        "\n",
        "# Combine classification and response generation into one chain and take forced actions\n",
        "\n",
        "\n",
        "# Run the chain with an example review\n",
        "\n",
        "# Good review - \"The product is excellent. I really enjoyed using it and found it very helpful.\"\n",
        "# Bad review - \"The product is terrible. It broke after just one use and the quality is very poor.\"\n",
        "# Neutral review - \"The product is okay. It works as expected but nothing exceptional.\"\n",
        "# Default - \"I'm not sure about the product yet. Can you tell me more about its features and benefits?\"\n",
        "\n",
        "\n",
        "\n",
        "# Output the result\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAWbgksrJ9iw"
      },
      "source": [
        "## Now that we have a good grasp of how LCEL works time to try Agents!\n",
        "\n",
        "#### Let's go to the slides to understand better how agents work and better understand it's usecase\n",
        "\n",
        "#### tools parameters, https://api.python.langchain.com/en/latest/agents/langchain.agents.load_tools.load_tools.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "h9p1hnOr0p0Q"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import load_tools, create_react_agent, initialize_agent, AgentType, AgentExecutor, create_structured_chat_agent\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from langchain_core.tools import Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "823VJcAZ3HWy",
        "outputId": "58a248c6-f494-4fe8-c153-eb72b7b4d4db"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "2_uEc-J41L0D",
        "outputId": "f952cf1a-a006-4d1c-d488-c95677df7e45"
      },
      "outputs": [],
      "source": [
        "# Play around with average and median\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8e0oWpHJ9ix"
      },
      "source": [
        "## LangChain Agent Deep Dive\n",
        "\n",
        "#### Understanding tools a little better\n",
        "#### Creating a chatbot using ReAct agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "rQhMkLWp6tv_"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "from wikipedia import summary\n",
        "from langchain import hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "1Hfq1RpJ5RZs"
      },
      "outputs": [],
      "source": [
        "# Define Tools DATETIME tool\n",
        "def get_current_time(*args, **kwargs):\n",
        "    \"\"\"Returns the current time in H:MM AM/PM format.\"\"\"\n",
        "\n",
        "    now = datetime.datetime.now()\n",
        "    return now.strftime(\"%I:%M %p\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "kALxhrWw5YQS"
      },
      "outputs": [],
      "source": [
        "# Search wikipedia tool\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "jwYhchIJ64lp"
      },
      "outputs": [],
      "source": [
        "# Define the tools that the agent can use\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqoI2Ft46-AC",
        "outputId": "29b5ac27-0072-4573-d19a-e121061e7d5e"
      },
      "outputs": [],
      "source": [
        "# Load the correct JSON Chat Prompt from the hub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSQlMjrK7R4r",
        "outputId": "80b781d1-5604-41c2-ecc3-a342f0159a99"
      },
      "outputs": [],
      "source": [
        "# Create a structured Chat Agent with Conversation Buffer Memory\n",
        "\n",
        "# ConversationBufferMemory stores the conversation history, allowing the agent to maintain context across interactions! Context help with his reasoning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "4J6FH-Ro7RyA"
      },
      "outputs": [],
      "source": [
        "# create_structured_chat_agent initializes a chat agent designed to interact using a structured prompt and tools\n",
        "# It combines the language model (llm), tools, and prompt to create an interactive agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "dOjS3SqM8ST0"
      },
      "outputs": [],
      "source": [
        "# AgentExecutor is responsible for managing the interaction between the user input, the agent, and the tools\n",
        "# It also handles memory to ensure context is maintained throughout the conversation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "BJ7h6P3R8SRf"
      },
      "outputs": [],
      "source": [
        "# Initial system message to set the context for the chat\n",
        "# SystemMessage is used to define a message from the system to the agent, setting initial instructions or context\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvgM9OZu8SOG",
        "outputId": "56156a23-2cba-4f22-874b-51784644c9e2"
      },
      "outputs": [],
      "source": [
        "# Chat Loop to interact with the user\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W34sUSaEJ9ix"
      },
      "source": [
        "### LangChain Tools Deep Dive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "gfCKH6Zy-6Y8"
      },
      "outputs": [],
      "source": [
        "# Docs: https://python.langchain.com/v0.1/docs/modules/tools/custom_tools/\n",
        "\n",
        "from langchain_core.tools import StructuredTool, Tool\n",
        "from langchain.agents import AgentExecutor, create_tool_calling_agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "xmcKBEDt-t0B"
      },
      "outputs": [],
      "source": [
        "def greet_user(name: str) -> str:\n",
        "    \"\"\"Greets the user by name.\"\"\"\n",
        "    return f\"Hello, {name}!\"\n",
        "\n",
        "\n",
        "def reverse_string(text: str) -> str:\n",
        "    \"\"\"Reverses the given string.\"\"\"\n",
        "    return text[::-1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "-YZ3Br3m-wrc"
      },
      "outputs": [],
      "source": [
        "# Pydantic model for tool arguments\n",
        "class ConcatenateStringsArgs(BaseModel):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Ck36j0il-0wJ"
      },
      "outputs": [],
      "source": [
        "# Create tools using the Tool and StructuredTool constructor approach\n",
        "tools = [\n",
        "    # Use Tool for simpler functions with a single input parameter.\n",
        "    # This is straightforward and doesn't require an input schema.\n",
        "    \n",
        "    # Use Tool for another simple function with a single input parameter.\n",
        "    \n",
        "    # Use StructuredTool for more complex functions that require multiple input parameters.\n",
        "    # StructuredTool allows us to define an input schema using Pydantic, ensuring proper validation and description.\n",
        "    \n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "P2ZOBmW-_n64"
      },
      "outputs": [],
      "source": [
        "# Pull the prompt template from the hub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "WP95ZDrY_qaq"
      },
      "outputs": [],
      "source": [
        "# Create the ReAct agent using the create_tool_calling_agent function\n",
        "agent = create_tool_calling_agent(\n",
        "# Language model to use\n",
        "# List of tools available to the agent\n",
        "# Prompt template to guide the agent's responses\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "sintmttW_uI_"
      },
      "outputs": [],
      "source": [
        "# Create the agent executor\n",
        "agent_executor = AgentExecutor.from_agent_and_tools(\n",
        "# The agent to execute\n",
        "# List of tools available to the agent\n",
        "# Enable verbose logging\n",
        "# Handle parsing errors gracefully\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5fcwRd0-3wg",
        "outputId": "2feaf0b2-24fe-4db5-b654-0a808d274674"
      },
      "outputs": [],
      "source": [
        "# Test the agent with sample queries\n",
        "response = agent_executor.invoke({\"input\": \"Greet Alice\"})\n",
        "print(\"Response for 'Greet Alice':\", response)\n",
        "\n",
        "response = agent_executor.invoke({\"input\": \"Reverse the string 'hello'\"})\n",
        "print(\"Response for 'Reverse the string hello':\", response)\n",
        "\n",
        "response = agent_executor.invoke({\"input\": \"Concatenate 'hello' and 'world'\"})\n",
        "print(\"Response for 'Concatenate hello and world':\", response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slgzYVY4J9ix"
      },
      "source": [
        "# Langgraph\n",
        "\n",
        "### LangGraph is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. Compared to other LLM frameworks, it offers these core benefits: cycles, controllability, and persistence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "4qd22BqPIklG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Get a list of 100 attractions in Singapore for tourists\",\n",
        "        },\n",
        "    ]\n",
        "\n",
        "response = gpt.invoke(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYVTp-2aInMS",
        "outputId": "28685552-4409-4675-e232-2011a2f3b2fd"
      },
      "outputs": [],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "W-qz_rUCI-wR"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\\\n",
        "    Please give more information about a travel destination.\n",
        "    please include address, category, recommendated visit duration and description\n",
        "    when you give the recommendation\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "s-gxHh7HJDhz"
      },
      "outputs": [],
      "source": [
        "messages=[\n",
        "\n",
        "]\n",
        "\n",
        "response = gpt.invoke(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "saNk_gBiJTh-",
        "outputId": "a2fcb64b-dfed-47a8-902d-4e4ebcea0f0d"
      },
      "outputs": [],
      "source": [
        "response.content.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "GJardR4xJjeA"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\\\n",
        "    Please give more information about a travel destination.\n",
        "    please include address, category, recommendated visit duration and description.\n",
        "    recommended visit duration should be in minutes.\n",
        "    when you give the recommendation.\n",
        "\n",
        "    Please make sure the response is in valid JSON format.\n",
        "\n",
        "    For example:\n",
        "    {\n",
        "        \"attractions\": [\n",
        "            {\n",
        "                \"name\": \"Marina Bay Sands\",\n",
        "                \"address\": \"10 Bayfront Ave, Singapore 018956\",\n",
        "                \"category\": \"Hotel\",\n",
        "                \"recommended_visit_duration\": \"120\",\n",
        "                \"description\": \"Marina Bay Sands is an integrated resort fronting Marina Bay in Singapore. At its opening in 2010, it was billed as the world's most expensive standalone casino property at S$8 billion, including the land cost.\"\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "PtS0s6E1Jn-8"
      },
      "outputs": [],
      "source": [
        "messages=[\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": prompt,\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Get a list of 100 attractions in Singapore for tourists\",\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXIaj8g5JrFU",
        "outputId": "76b4d5db-44dd-409c-dba2-113ddb3e2d53"
      },
      "outputs": [],
      "source": [
        "response = gpt.invoke(messages)\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vuswxg3gKttl"
      },
      "source": [
        "We want 100 places, but the LLM only give us partial result\n",
        "\n",
        "how to solve this problem?\n",
        "\n",
        "1. ask fewer each time, ask for 20 items, and repeated with context\n",
        "1. generate 100 names first and ask for the descriptions and so on for each of them\n",
        "\n",
        "![Attractions Workflow](https://i.imgur.com/fuyiGCs.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "tzz3Bx6hJ9ix"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    # input is like \"100, Singapore\"\n",
        "    input: str\n",
        "    num: int\n",
        "    tasks: list\n",
        "    places: list\n",
        "    current_places: list\n",
        "    output: str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "F0S5UnzhJ9ix"
      },
      "outputs": [],
      "source": [
        "class Attraction(BaseModel):\n",
        "    name: str = Field(description=\"Name of the attraction\")\n",
        "    address: str = Field(description=\"Address of the attraction\")\n",
        "    category: str = Field(description=\"Category of the attraction\")\n",
        "    recommended_visit_duration: int = Field(\n",
        "        description=\"Recommended visit duration in minutes\"\n",
        "    )\n",
        "    description: str = Field(description=\"Description of the attraction\")\n",
        "\n",
        "\n",
        "class Attractions(BaseModel):\n",
        "    places: list[Attraction] = Field(description=\"List of attractions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "RCRLgoc4hurc"
      },
      "outputs": [],
      "source": [
        "def generate_task(state: State):\n",
        "    \"\"\"break the task into smaller tasks\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esBHe_f5hxg9",
        "outputId": "e1e14a9e-00b6-4fad-b673-f69d2cc31c2d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "dtg_OPAuhxfJ"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a professional travel planner. Please generate the attractions based on the task and do not repeat from the already known places.\",\n",
        "        ),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"\"\"\\\n",
        "                Here is the task:\n",
        "                {task}\n",
        "\n",
        "                Here are the places I have already known, please don't repeat them:\n",
        "                {places}\n",
        "                \"\"\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "UNUP_nlWhxcO"
      },
      "outputs": [],
      "source": [
        "results = executor.invoke(\n",
        "    {\n",
        "        \"task\": \"Generate a list of 19 attractions in Singapore for tourists\",\n",
        "        \"places\": [],\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5O4MMUUi6-W",
        "outputId": "65d660cb-3326-4e4f-fe66-980b10c8713e"
      },
      "outputs": [],
      "source": [
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# We need to handle the duplication!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "f8i5Z7rah5U8"
      },
      "outputs": [],
      "source": [
        "# Creating a state machine, just workflow\n",
        "def execute(state: State):\n",
        "    \"\"\"call the LLM to perform the task\"\"\"\n",
        "    # 1 get one task from the tasks, the pop it\n",
        "    # 2 call the LLM to perform the task and get the result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "kA2qkqSDh5Lt"
      },
      "outputs": [],
      "source": [
        "dedup_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant in deduplicating the attractions. Please remove the duplicates from the current places and add them to the known places.\",\n",
        "        ),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"\"\"\\\n",
        "                Here are the places already known:\n",
        "                {places}\n",
        "\n",
        "                Here are the newly generated places:\n",
        "                {current_places}\n",
        "\n",
        "                Return the current places without duplicates.\n",
        "                \"\"\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "vPoyQOxoh5JO"
      },
      "outputs": [],
      "source": [
        "def dedup(state: State):\n",
        "    \"\"\"\"\"\"\n",
        "    # call the LLM to deduplicate the places\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "eq8hoAfzh5GK"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "f7ijB_fTiGPs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75EY3DH6J9ix"
      },
      "source": [
        "### Langserve! If time permit!\n",
        "\n",
        "**LangServe** is a tool within LangChain that makes it easy to deploy your language model applications in production. It allows you to serve your LangChain workflows as APIs, which is super helpful when you want to scale or integrate your application into larger systems.\n",
        "\n",
        "---\n",
        "\n",
        "### Why is it helpful?\n",
        "- **Easy Deployment**: LangServe simplifies the process of turning your LangChain workflows into APIs.\n",
        "- **Scalability**: It helps scale your application so that multiple users or systems can interact with it.\n",
        "- **Seamless Integration**: LangServe allows you to connect your application with other services through an API, making it accessible in different environments.\n",
        "\n",
        "---\n",
        "\n",
        "### When should you use it?\n",
        "- When you need to **deploy** your LangChain project for **real-world use**.\n",
        "- If you want to **share your model** as a service for others to use (e.g., for a web app or another application).\n",
        "- When you need to **scale** your language model applications to handle more users or requests.\n",
        "\n",
        "In short, LangServe is perfect for moving your LangChain project from development to production!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "_FS-AFiuAo-Y"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langserve import add_routes\n",
        "import nest_asyncio\n",
        "import uvicorn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "DrawNJoXAyy5"
      },
      "outputs": [],
      "source": [
        "# 1. Create prompt template\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "ly4-Se1dA5gB"
      },
      "outputs": [],
      "source": [
        "# 2. Create chain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "lTA6PWwfA9r6"
      },
      "outputs": [],
      "source": [
        "# 3. App definition\n",
        "app = FastAPI(\n",
        "  title=\"LangChain Server\",\n",
        "  version=\"1.0\",\n",
        "  description=\"A simple API server using LangChain's Runnable interfaces\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "9DiU8tlVBfvd"
      },
      "outputs": [],
      "source": [
        "# Allow asyncio to work within a running event loop (e.g., in Jupyter)\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "jnsUczlhHVVI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "8r5FB9-lBBuX",
        "outputId": "5d14b6d3-0fe3-4bcf-c858-fc6f11dbd2e6"
      },
      "outputs": [],
      "source": [
        "# 4. Adding chain route\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
