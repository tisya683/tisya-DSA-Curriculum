{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download all the necessary libraries we will need for today session\n",
    "\n",
    "#### 1) We will try out using \n",
    "    pip install pipenv\n",
    "    pipenv install\n",
    "    pipenv shell\n",
    "If dependancy is still an issue we will go to Google Colab.\n",
    "#### 2) Download the libraries we will need for today's session\n",
    "#### 3) Load in our API credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be running the following code in our terminal/ cell depending on what is chosen.\n",
    "\n",
    "# Our own local machine:\n",
    "# %pipenv install langchain langchain_core langchain_openai langchain-community langchain_text_splitters openai pypdf python-dotenv numpy scikit-learn matplotlib scikit-learn scipy beautifulsoup4 pypdf chroma langgraph typing typing_extensions \n",
    "\n",
    "# Google Colab:\n",
    "# %pip install langchain langchain_core langchain_openai langchain-community langchain_text_splitters openai pypdf python-dotenv numpy scikit-learn matplotlib scikit-learn scipy beautifulsoup4 pypdf chroma langgraph typing typing_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries needed\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"test.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If LangSmith isn't tracking try to restart the entire kernel at the top\n",
    "trace = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "langsmith = os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model using GPT-4\n",
    "gpt = ChatOpenAI(\n",
    "    model='gpt-4o',\n",
    "    openai_api_key = os.getenv(\"OPENAI_API_KEY\"),\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test it out\n",
    "results = gpt.invoke(\"Tell me something about singapore that most singaporeans wouldn't know about.\")\n",
    "print(results.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal Embeddings models\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings models\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings2 = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    dimensions = 1024\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Data Analytics Club is dope and fun, in today's session we will be covering topics on GenAI, specifically RAG systems\"\n",
    "\n",
    "# Embed the text above using the embed_query function\n",
    "\n",
    "\n",
    "# Represent the semantic meaning of the text, or in other words the vector and position in a given vector space.\n",
    "\n",
    "\n",
    "# Dimensionality\n",
    "print(len(single_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduced dimensionality, incase your computer cannot handle the computation required\n",
    "# Improves model performance and reduces overfitting\n",
    "# For visualisation and human interpretations\n",
    "\n",
    "single_vector2 = embeddings2.embed_query(text)\n",
    "print(single_vector2)\n",
    "\n",
    "print(len(single_vector2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings on a larger text/documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of products in a shop\n",
    "\n",
    "products = [\n",
    "    {\n",
    "        'title': 'Smartphone X1',\n",
    "        'short_description': 'The latest flagship smartphone with AI-powered features and 5G connectivity.',\n",
    "        'price': 799.99,\n",
    "        'category': 'Electronics',\n",
    "        'features': [\n",
    "            '6.5-inch AMOLED display',\n",
    "            'Quad-camera system with 48MP main sensor',\n",
    "            'Face recognition and fingerprint sensor',\n",
    "            'Fast wireless charging'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'title': 'Luxury Diamond Necklace',\n",
    "        'short_description': 'Elegant necklace featuring genuine diamonds, perfect for special occasions.',\n",
    "        'price': 1499.99,\n",
    "        'category': 'Beauty',\n",
    "        'features': [\n",
    "            '18k white gold chain',\n",
    "            '0.5 carat diamond pendant',\n",
    "            'Adjustable chain length',\n",
    "            'Gift box included'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'title': 'RC Racing Car',\n",
    "        'short_description': 'High-speed remote-controlled racing car for adrenaline-packed fun.',\n",
    "        'price': 89.99,\n",
    "        'category': 'Toys',\n",
    "        'features': [\n",
    "            'Top speed of 30 mph',\n",
    "            'Responsive remote control',\n",
    "            'Rechargeable battery',\n",
    "            'Durable construction'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'title': 'Ultra HD 4K TV',\n",
    "        'short_description': 'Immerse yourself in stunning visuals with this 65-inch 4K TV.',\n",
    "        'price': 1299.99,\n",
    "        'category': 'Electronics',\n",
    "        'features': [\n",
    "            '65-inch 4K UHD display',\n",
    "            'Dolby Vision and HDR10+ support',\n",
    "            'Smart TV with streaming apps',\n",
    "            'Voice remote included'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'title': 'Glowing Skin Serum',\n",
    "        'short_description': 'Revitalize your skin with this nourishing serum for a radiant glow.',\n",
    "        'price': 39.99,\n",
    "        'category': 'Beauty',\n",
    "        'features': [\n",
    "            'Hyaluronic acid and vitamin C',\n",
    "            'Hydrates and reduces fine lines',\n",
    "            'Suitable for all skin types',\n",
    "            'Cruelty-free'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'title': 'LEGO Space Shuttle',\n",
    "        'short_description': 'Build your own space adventure with this LEGO space shuttle set.',\n",
    "        'price': 49.99,\n",
    "        'category': 'Toys',\n",
    "        'features': [\n",
    "            '359 pieces for creative building',\n",
    "            'Astronaut minifigure included',\n",
    "            'Compatible with other LEGO sets',\n",
    "            'For ages 7+'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'title': 'Wireless Noise-Canceling Headphones',\n",
    "        'short_description': 'Enjoy immersive audio and block out distractions with these headphones.',\n",
    "        'price': 199.99,\n",
    "        'category': 'Electronics',\n",
    "        'features': [\n",
    "            'Active noise cancellation',\n",
    "            'Bluetooth 5.0 connectivity',\n",
    "            'Long-lasting battery life',\n",
    "            'Foldable design for portability'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'title': 'Luxury Perfume Gift Set',\n",
    "        'short_description': 'Indulge in a collection of premium fragrances with this gift set.',\n",
    "        'price': 129.99,\n",
    "        'category': 'Beauty',\n",
    "        'features': [\n",
    "            'Five unique scents',\n",
    "            'Elegant packaging',\n",
    "            'Perfect gift for fragrance enthusiasts',\n",
    "            'Variety of fragrance notes'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'title': 'Remote-Controlled Drone',\n",
    "        'short_description': 'Take to the skies and capture stunning aerial footage with this drone.',\n",
    "        'price': 299.99,\n",
    "        'category': 'Electronics',\n",
    "        'features': [\n",
    "            '4K camera with gimbal stabilization',\n",
    "            'GPS-assisted flight',\n",
    "            'Remote control with smartphone app',\n",
    "            'Return-to-home function'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'title': 'Luxurious Spa Gift Basket',\n",
    "        'short_description': 'Pamper yourself or a loved one with this spa gift basket full of relaxation goodies.',\n",
    "        'price': 79.99,\n",
    "        'category': 'Beauty',\n",
    "        'features': [\n",
    "            'Bath bombs, body lotion, and more',\n",
    "            'Aromatherapy candles',\n",
    "            'Reusable wicker basket',\n",
    "            'Great for self-care'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'title': 'Robot Building Kit',\n",
    "        'short_description': 'Learn robotics and coding with this educational robot building kit.',\n",
    "        'price': 59.99,\n",
    "        'category': 'Toys',\n",
    "        'features': [\n",
    "            'Build and program your own robot',\n",
    "            'STEM learning tool',\n",
    "            'Compatible with Scratch and Python',\n",
    "            'Ideal for young inventors'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'title': 'High-Performance Gaming Laptop',\n",
    "        'short_description': 'Dominate the gaming world with this powerful gaming laptop.',\n",
    "        'price': 1499.99,\n",
    "        'category': 'Electronics',\n",
    "        'features': [\n",
    "            'Intel Core i7 processor',\n",
    "            'NVIDIA RTX graphics',\n",
    "            '144Hz refresh rate display',\n",
    "            'RGB backlit keyboard'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'title': 'Natural Mineral Makeup Set',\n",
    "        'short_description': 'Enhance your beauty with this mineral makeup set for a flawless look.',\n",
    "        'price': 34.99,\n",
    "        'category': 'Beauty',\n",
    "        'features': [\n",
    "            'Mineral foundation and eyeshadows',\n",
    "            'Non-comedogenic and paraben-free',\n",
    "            'Cruelty-free and vegan',\n",
    "            'Includes makeup brushes'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'title': 'Interactive Robot Pet',\n",
    "        'short_description': 'Adopt your own robot pet that responds to your voice and touch.',\n",
    "        'price': 79.99,\n",
    "        'category': 'Toys',\n",
    "        'features': [\n",
    "            'Realistic pet behaviors',\n",
    "            'Voice recognition and touch sensors',\n",
    "            'Teaches responsibility and empathy',\n",
    "            'Rechargeable battery'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'title': 'Smart Thermostat',\n",
    "        'short_description': \"Control your home's temperature and save energy with this smart thermostat.\",\n",
    "        'price': 129.99,\n",
    "        'category': 'Electronics',\n",
    "        'features': [\n",
    "            'Wi-Fi connectivity',\n",
    "            'Energy-saving features',\n",
    "            'Compatible with voice assistants',\n",
    "            'Easy installation'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'title': 'Designer Makeup Brush Set',\n",
    "        'short_description': 'Upgrade your makeup routine with this premium designer brush set.',\n",
    "        'price': 59.99,\n",
    "        'category': 'Beauty',\n",
    "        'features': [\n",
    "            'High-quality synthetic bristles',\n",
    "            'Chic designer brush handles',\n",
    "            'Complete set for all makeup needs',\n",
    "            'Includes stylish carrying case'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'title': 'Remote-Controlled Dinosaur Toy',\n",
    "        'short_description': 'Roar into action with this remote-controlled dinosaur toy with lifelike movements.',\n",
    "        'price': 49.99,\n",
    "        'category': 'Toys',\n",
    "        'features': [\n",
    "            'Realistic dinosaur sound effects',\n",
    "            'Walks and roars like a real dinosaur',\n",
    "            'Remote control included',\n",
    "            'Educational and entertaining'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'title': 'Wireless Charging Dock',\n",
    "        'short_description': 'Charge your devices conveniently with this sleek wireless charging dock.',\n",
    "        'price': 39.99,\n",
    "        'category': 'Electronics',\n",
    "        'features': [\n",
    "            'Qi wireless charging technology',\n",
    "            'Supports multiple devices',\n",
    "            'LED charging indicators',\n",
    "            'Compact and stylish design'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'title': 'Luxury Skincare Set',\n",
    "        'short_description': 'Elevate your skincare routine with this luxurious skincare set.',\n",
    "        'price': 179.99,\n",
    "        'category': 'Beauty',\n",
    "        'features': [\n",
    "            'Premium anti-aging ingredients',\n",
    "            'Hydrating and rejuvenating formulas',\n",
    "            'Complete skincare regimen',\n",
    "            'Elegant packaging'\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type of keys we have\n",
    "print(products[0].keys())\n",
    "\n",
    "# Number of products being sold in our outlet\n",
    "print(len(products))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to embed the short descriptions from products\n",
    "# Extract a list of product short descriptions from products\n",
    "\n",
    "\n",
    "print(product_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings for each product description, nested list\n",
    "\n",
    "\n",
    "\n",
    "# We will use embed_documents here \n",
    "print(response)\n",
    "print(len(response))\n",
    "\n",
    "# The whole thing will be embedded as a whole\n",
    "print(response2)\n",
    "print(len(response2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the embeddings from response_dict and store in products\n",
    "\n",
    "\n",
    "print(products[0])\n",
    "print(products[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "#### 1) Understanding how the vectors work in a way that humans can understand\n",
    "\n",
    "    a) There are two ways of performing a dimensionality reduction task. Linear and Non-linear reduction methods.\n",
    "    b) We will be using PCA and TSNE for our visualisation purposes\n",
    "\n",
    "##### 2) Let's go back to the slides to understand abit more about dimensionality reduction and the techniques to accomplish them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-distributed Stochastic Neighbor Embedding (TSNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create reviews and embeddings lists using list comprehensions\n",
    "categories = [product['category'] for product in products]\n",
    "embeddings = [product['embeddings'] for product in products]\n",
    "\n",
    "# Reduce the number of embeddings dimensions to two using t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=5)\n",
    "embeddings_2d = tsne.fit_transform(np.array(embeddings))\n",
    "\n",
    "# Create a scatter plot of the 2D embeddings\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Create a scatter plot from embeddings_2d\n",
    "plt.scatter(embeddings_2d[:,0], embeddings_2d[:,1], alpha=0.7)\n",
    "\n",
    "for i, category in enumerate(categories):\n",
    "    plt.annotate(category, (embeddings_2d[i, 0], embeddings_2d[i, 1]))\n",
    "\n",
    "plt.xlabel(\"TSNE Component 1\")\n",
    "plt.ylabel(\"TSNE Component 2\")\n",
    "plt.title(\"2D Visualization of Product Embeddings by Category\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract categories and embeddings from products\n",
    "categories = [product['category'] for product in products]\n",
    "embeddings = [product['embeddings'] for product in products]  # Assuming 'embedding' contains vector data\n",
    "\n",
    "# Reduce dimensions to 2D using PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(embeddings)\n",
    "\n",
    "# Create a scatter plot of the 2D embeddings\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Numpy way of finding every row and column to pick from.\n",
    "plt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.7)\n",
    "\n",
    "# Annotate each point with its category\n",
    "for i, category in enumerate(categories):\n",
    "    plt.annotate(category, (pca_result[i, 0], pca_result[i, 1]), fontsize=9, alpha=0.8)\n",
    "\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.title(\"2D Visualization of Product Embeddings by Category\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Search\n",
    "    We embedded the text/documents and received our vectors. \n",
    "    Now what how can our machine find the information we are looking for?\n",
    "    Using similarity search!\n",
    "### There are three ways to go about finding similar information/vectors\n",
    "    1) Euclidean Distance\n",
    "    2) Cosine Similarity\n",
    "    3) Inner Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A more mathematical approach of finding the cosine distance of each vector\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# Embed the search text\n",
    "search_text = \"soap\"\n",
    "\n",
    "\n",
    "# Compute the cosine distance for each product description\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Find the index of the minimum value in an array\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Find and print the most similar product short_description    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VectorStore and Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Cats are independent pets that often enjoy their own space.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Goldfish are popular pets for beginners, requiring relatively simple care.\",\n",
    "        metadata={\"source\": \"fish-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Parrots are intelligent birds capable of mimicking human speech.\",\n",
    "        metadata={\"source\": \"bird-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Rabbits are social animals that need plenty of space to hop around.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardising from now onwards we will only use embedding variable\n",
    "embedding = OpenAIEmbeddings(\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model=\"text-embedding-3-large\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "#Create our Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use similarity_search when we have a certain text we are looking for\n",
    "\n",
    "\n",
    "# We can also indicate how many similar results we want to get back\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use similarity_search when we have a certain text we are looking for\n",
    "# We can also indicate how many similar results we want to get back\n",
    "\n",
    "# Euclidean, data between 0 to 2\n",
    "print(vectorstore.similarity_search_with_score(\"cat\"))\n",
    "\n",
    "\n",
    "# Cosine, data between 0 to 1\n",
    "print(vectorstore.similarity_search_with_relevance_scores(\"cat\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other search methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrievers\n",
    "    Above we created a retriever object if we want to incorporate the LCEL we need to make it into a runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create runnables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "system_message = \"\"\"\n",
    "Answer this question using the provided context only.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation (RAG) Systems\n",
    "    Retrieval Augmented Generation (RAG) systems combine retrieval-based and generative approaches to enhance AI responses. In a RAG system, the model first retrieves relevant information (documents or text) from a large dataset based on the query. It then generates answers using this retrieved context, ensuring responses are accurate and contextually grounded. This method improves upon traditional generative models by allowing the AI to access up-to-date knowledge and provide more factual and relevant answers, making it ideal for applications like chatbots, search engines, and question-answering systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look at a full rag process\n",
    "\n",
    "[Link to the Nike download document](https://drive.google.com/file/d/1sGGBRseXw3lTecdeHt7bBNzgnn9eClJ8/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PDF into our notebook\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"414759-1-_5_Nike-NPS-Combo_Form-10-K_WR.pdf\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in docs[:4]:\n",
    "    print(i)\n",
    "    print(\"------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Embedding Model Comparison\n",
    "\n",
    "| Model Name                | Dimensions | MIRACL Avg. Score | MTEB Avg. Score | Context Token Limit | Price per 1k Tokens       |\n",
    "|---------------------------|------------|--------------------|-----------------|----------------------|---------------------------|\n",
    "| **text-embedding-ada-002** | 1536       | 31.4%             | 61.0%          | 8,192 tokens        | $0.0001                   |\n",
    "| **text-embedding-3-small** | 1536       | 44.0%             | 62.3%          | 8,192 tokens        | Not Specified             |\n",
    "| **text-embedding-3-large** | 3072       | 54.9%             | 64.6%          | 8,192 tokens        | $0.00013                  |\n",
    "\n",
    "**text-embedding-3-large** is the latest and best-performing model, offering improved accuracy over **text-embedding-ada-002**:\n",
    "- **MIRACL Score**: 31.4% to 54.9% improvement.\n",
    "- **MTEB Score**: 61.0% to 64.6% improvement.\n",
    "\n",
    "For more details, see the [Embeddings Guide](https://platform.openai.com/docs).\n",
    "\n",
    "--- \n",
    "\n",
    "Stronger performance. text-embedding-3-large is our new best performing model. Comparing text-embedding-ada-002 to text-embedding-3-large: on MIRACL, the average score has increased from 31.4% to 54.9%, while on MTEB, the average score has increased from 61.0% to 64.6%.\n",
    "Here’s the table with the **Context Token Limit** added for each model:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why we need to chunk/split\n",
    "def count_tokens(text, model_name=\"text-embedding-ada-002\"):\n",
    "    # Initialize the tokenizer for the specified model\n",
    "    encoding = tiktoken.encoding_for_model(model_name)\n",
    "    \n",
    "    # Encode the text and count the tokens\n",
    "    tokens = encoding.encode(text)\n",
    "    token_count = len(tokens)\n",
    "    \n",
    "    return token_count\n",
    "\n",
    "# Example document\n",
    "document = docs[1].page_content\n",
    "\n",
    "# Check token count\n",
    "token_limit = 8192  # Example limit for `text-embedding-ada-002`\n",
    "token_count = count_tokens(document)\n",
    "\n",
    "print(f\"Token Count: {token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hence we have to split into different chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our prompt template\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for result in results:\n",
    "    print(result[\"context\"][0].page_content)\n",
    "    print(\"------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's build a conversational bot using RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our necessary websites\n",
    "url1 = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
    "response1 = requests.get(url1)\n",
    "\n",
    "url2 = 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/'\n",
    "response2 = requests.get(url2)\n",
    "\n",
    "print(response1) # 200 good, others no good\n",
    "print(response2) # 200 good, others no good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load, chunk and index the contents of the blog to create a retriever.\n",
    "\n",
    "# \"https://lilianweng.github.io/posts/2023-06-23-agent/\", \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just incase the top code messed up\n",
    "embedding = OpenAIEmbeddings(\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model=\"text-embedding-3-large\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Split that document up due to LLM's context window\n",
    "\n",
    "# Create a retriever that internally performs a similarity search \n",
    "# Cosine similarity search is performed under the hood when we use vectorstore but other databases might use something different\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create the system prompt, human prompt and ChatPromptTemplate\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Incorporate the retriever into a question-answering chain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Prompting for an answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Chat history\n",
    "    User queries may require conversational context to understand\n",
    "\n",
    "``` bash\n",
    "Human: \"What is Task Decomposition?\"\n",
    "AI: \"Task decomposition involves breaking down complex tasks into smaller and simpler steps to make them more manageable for an agent or model.\"\n",
    "Human: \"What are common ways of doing it?\"\n",
    "In order to answer the second question, our system needs to understand that \"it\" refers to \"Task Decomposition.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rephrasing the input received to have more information\n",
    "contextualise_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as it is.\"\n",
    ")\n",
    "\n",
    "# Update our prompt to support historical messages as an input\n",
    "\n",
    "\n",
    "# Addition of a subchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New prompt template for UX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output with conversation history\n",
    "    Common ways of task decomposition include: (1) Using simple prompting with an LLM, such as asking for steps or subgoals, (2) Employing task-specific instructions tailored to the task at hand, and (3) Incorporating human inputs to guide the decomposition process. These approaches help break down complex tasks into smaller, more manageable steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangGraph\n",
    "    We are still manually coding our sequence out. Which is lame\n",
    "    LangGraph implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.\n",
    "    Wrapping our chat model in a minimal LangGraph application allows us to automatically persist the message history, simplifying the development of multi-turn applications.\n",
    "\n",
    "#### What LangGraph is trying to solve here\n",
    "``` bash\n",
    "Modularization of Each Component:\n",
    "A RAG system typically involves steps like retrieving documents, embedding queries, generating prompts, and answering queries. Ours also have chat history.\n",
    "LangGraph lets you modularize each of these tasks into distinct nodes, \n",
    "making the system easier to manage and modify without affecting the entire workflow.\n",
    "```\n",
    "\n",
    "``` bash\n",
    "Seamless Data Flow Between Steps:\n",
    "In a RAG system, data flows from user input (query) to document retrieval, then through embedding and prompt crafting, and finally into response generation. \n",
    "LangGraph manages this data flow, ensuring the right outputs feed into the correct next steps, which is essential for conversation continuity.\n",
    "```\n",
    "\n",
    "``` bash\n",
    "Testing and Experimentation at Different Stages:\n",
    "LangGraph enables you to experiment with different components (e.g., testing different retrieval methods or language models) \n",
    "without disrupting the entire pipeline. This is critical in RAG systems, where various modules affect response quality.\n",
    "```\n",
    "\n",
    "``` bash\n",
    "Error Handling and Debugging:\n",
    "With multiple components, errors can occur at any stage. \n",
    "LangGraph allows isolated troubleshooting of each step, making debugging more manageable. \n",
    "For instance, you can test retrieval independently if there’s a retrieval issue, without examining the generation phase.\n",
    "```\n",
    "\n",
    "``` bash\n",
    "Scalability for Multiple Query Types:\n",
    "Conversational RAG systems may need different responses based on context, such as follow-up questions or clarifications. \n",
    "LangGraph helps structure workflows that can handle multiple query types or dynamic branching within a single pipeline, making it easier to scale conversation management.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a dict representing the state of our conversational rag system.\n",
    "# This state has the same input and output keys as `rag_chain`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then define a simple node that runs the `rag_chain`.\n",
    "# The `return` values of the node update the graph state, so here we just\n",
    "# update the chat history with the input message and response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our graph only consist of one node:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we compile the graph with a checkpointer object.\n",
    "# This persists the state, in this case in memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config Dictionary\n",
    "    Manage multiple conversation threads within the application. By assigning a unique identifier (thread_id) to each thread, \n",
    "    this setup allows the application to keep track of separate conversation contexts—meaning\n",
    "    it can handle multiple users or ongoing threads independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up unique personnel chat\n",
    "\n",
    "\n",
    "\n",
    "print(result.keys())\n",
    "print(result[\"input\"])\n",
    "print(result[\"chat_history\"])\n",
    "print(result[\"context\"])\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing our history\n",
    "\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for message in chat_history:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entire Workflow for our Mini Conversational RAG bot\n",
    "![Description of Image](https://i.imgur.com/e1qlcWy.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Construct retriever ###\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(url1, url2),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = InMemoryVectorStore.from_documents(\n",
    "    documents=splits, embedding=embedding\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "### Contextualize question ###\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    gpt, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "\n",
    "### Answer question ###\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(gpt, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "\n",
    "### Statefully manage chat history ###\n",
    "class State(TypedDict):\n",
    "    input: str\n",
    "    chat_history: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    context: str\n",
    "    answer: str\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    response = rag_chain.invoke(state)\n",
    "    return {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(state[\"input\"]),\n",
    "            AIMessage(response[\"answer\"]),\n",
    "        ],\n",
    "        \"context\": response[\"context\"],\n",
    "        \"answer\": response[\"answer\"],\n",
    "    }\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema=State)\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "result = app.invoke(\n",
    "    {\"input\": \"What is Token manipulation?\"},\n",
    "    config=config,\n",
    ")\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = app.invoke(\n",
    "    {\"input\": \"How can we prevent that?\"},\n",
    "    config=config,\n",
    ")\n",
    "print(result[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSA-008_RAG_Systems-Gc4x7UMw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
