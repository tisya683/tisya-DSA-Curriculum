{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZooFTWZo8eX"
      },
      "source": [
        "# DSA Deep Learning [2] - Advanced CNNs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import display, HTML\n",
        "from PIL import Image\n",
        "from IPython.display import display, HTML, Javascript\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import base64\n",
        "import io\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab import output\n",
        "import os\n",
        "from tensorflow.keras.models import load_model"
      ],
      "metadata": {
        "id": "3HC-1Ua0Z33_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the FER2013 CSV file from Google Colab's local storage\n",
        "csv_path = 'fer2013.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Display the first few rows of the dataset to understand its structure\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "OvrsTPTKqzQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 2: Convert Pixel Data to Images\n",
        "# The FER2013 dataset typically has 'emotion' and 'pixels' columns\n",
        "df['pixels'] = df['pixels'].apply(lambda x: np.array(x.split(), dtype='float32'))\n",
        "X = np.stack(df['pixels'].values)  # Convert to a 2D array\n",
        "X = X.reshape(-1, 48, 48, 1)  # Reshape to (n_samples, 48, 48, 1) for grayscale images\n",
        "X = np.repeat(X, 3, axis=-1)  # Convert grayscale to RGB by duplicating channels\n",
        "\n",
        "# Normalize the pixel values\n",
        "X = X / 255.0\n",
        "\n",
        "# Convert labels to categorical format\n",
        "y = to_categorical(df['emotion'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "UilcIkNwZ9FN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 3: Building the Model\n",
        "# Using MobileNetV2 for transfer learning\n",
        "base_model = MobileNetV2(input_shape=(48, 48, 3), include_top=False, weights='imagenet')\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "predictions = Dense(7, activation='softmax')(x)  # FER2013 has 7 emotion classes\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the base layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "vSsQsFGfaBp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 4: Training the Model\n",
        "# Train the model with a small number of epochs\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=15,  # Limited to 15 epochs\n",
        "    validation_data=(X_val, y_val)\n",
        ")\n"
      ],
      "metadata": {
        "id": "IG6_q_BwaD-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model after training\n",
        "# Assuming `model` is the trained model from the training process\n",
        "model.save('emotion_recognition_model.h5')\n",
        "print(\"Model saved as 'emotion_recognition_model.h5'\")\n"
      ],
      "metadata": {
        "id": "vUTbrbZ0kWmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the model file exists and load it, otherwise train a new model\n",
        "model_path = 'emotion_recognition_model.h5'\n",
        "\n",
        "if os.path.exists(model_path):\n",
        "    print(\"Loading saved model...\")\n",
        "    model = load_model(model_path)\n",
        "    print(\"Model loaded successfully.\")\n",
        "else:\n",
        "    print(\"No saved model found. Please train the model.\")\n",
        "    # Code for building and training the model goes here (from previous training blocks)"
      ],
      "metadata": {
        "id": "d-onMA5frqg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a dictionary for emotion labels based on FER2013 class order\n",
        "emotion_labels = {\n",
        "    0: \"Angry\",\n",
        "    1: \"Disgust\",\n",
        "    2: \"Fear\",\n",
        "    3: \"Happy\",\n",
        "    4: \"Sad\",\n",
        "    5: \"Surprise\",\n",
        "    6: \"Neutral\"\n",
        "}\n",
        "\n",
        "# Updated predict_emotion function to handle multiple faces\n",
        "def predict_emotion(frame, model):\n",
        "    # Convert to grayscale for face detection\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Load the face detection model (Haar Cascade)\n",
        "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "    # Detect multiple faces in the frame\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(48, 48))\n",
        "\n",
        "    # Process each detected face\n",
        "    for (x, y, w, h) in faces:\n",
        "        # Extract the face region from the frame\n",
        "        face = frame[y:y+h, x:x+w]\n",
        "\n",
        "        # Resize face region to 48x48, the input size expected by the model\n",
        "        face_resized = cv2.resize(face, (48, 48))\n",
        "\n",
        "        # Preprocess face (normalize and add batch dimension)\n",
        "        face_array = np.expand_dims(face_resized, axis=0) / 255.0  # Scale pixel values to [0, 1]\n",
        "\n",
        "        # Predict emotion\n",
        "        emotion_prediction = model.predict(face_array)\n",
        "        emotion = np.argmax(emotion_prediction)  # Get the emotion class with the highest probability\n",
        "\n",
        "        # Draw a circle around the face and add the emotion label\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "        emotion_label = emotion_labels[emotion]  # Map the predicted emotion index to label\n",
        "        cv2.putText(frame, emotion_label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "    return frame\n"
      ],
      "metadata": {
        "id": "closxhY9fpik"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JavaScript code to start the live webcam feed and capture image upon button click\n",
        "def start_webcam_feed():\n",
        "    js = \"\"\"\n",
        "    <script>\n",
        "        let videoElement = null;\n",
        "        let stream = null;\n",
        "\n",
        "        async function startVideo() {\n",
        "            if (!videoElement) {\n",
        "                videoElement = document.createElement('video');\n",
        "                videoElement.setAttribute('autoplay', '');\n",
        "                videoElement.setAttribute('playsinline', '');\n",
        "                document.body.appendChild(videoElement);\n",
        "                stream = await navigator.mediaDevices.getUserMedia({ video: true });\n",
        "                videoElement.srcObject = stream;\n",
        "            }\n",
        "        }\n",
        "\n",
        "        async function capturePhoto() {\n",
        "            const canvas = document.createElement('canvas');\n",
        "            canvas.width = videoElement.videoWidth;\n",
        "            canvas.height = videoElement.videoHeight;\n",
        "            canvas.getContext('2d').drawImage(videoElement, 0, 0);\n",
        "\n",
        "            // Stop video feed\n",
        "            stream.getTracks().forEach(track => track.stop());\n",
        "            videoElement.remove();\n",
        "            videoElement = null;\n",
        "\n",
        "            // Convert the photo to base64 and send to Python\n",
        "            const dataUrl = canvas.toDataURL('image/jpeg');\n",
        "            google.colab.kernel.invokeFunction('notebook.get_webcam_image', [dataUrl], {});\n",
        "        }\n",
        "\n",
        "        // Add the start and capture buttons to the DOM\n",
        "        const startButton = document.createElement('button');\n",
        "        startButton.innerHTML = 'Start Webcam Feed';\n",
        "        startButton.onclick = startVideo;\n",
        "        document.body.appendChild(startButton);\n",
        "\n",
        "        const captureButton = document.createElement('button');\n",
        "        captureButton.innerHTML = 'Capture Photo';\n",
        "        captureButton.onclick = capturePhoto;\n",
        "        document.body.appendChild(captureButton);\n",
        "    </script>\n",
        "    \"\"\"\n",
        "    display(HTML(js))\n",
        "\n",
        "# Callback function to receive the captured image in Python\n",
        "def get_webcam_image(dataUrl):\n",
        "    # Decode the base64 image data\n",
        "    img_data = base64.b64decode(dataUrl.split(\",\")[1])\n",
        "    img = Image.open(io.BytesIO(img_data))\n",
        "\n",
        "    # Convert the image to OpenCV format\n",
        "    img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Perform emotion detection and display the processed image\n",
        "    processed_img = predict_emotion(img, model)\n",
        "    cv2_imshow(processed_img)  # Display the processed image with predictions\n",
        "\n",
        "# Register the callback function in Google Colab\n",
        "output.register_callback('notebook.get_webcam_image', get_webcam_image)\n",
        "\n",
        "# Initialize the webcam feed and buttons\n",
        "start_webcam_feed()\n"
      ],
      "metadata": {
        "id": "2LJojW38fr5q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}